{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np \n",
    "from keras.utils import to_categorical\n",
    "from validation import compute_f1\n",
    "from keras.models import Model\n",
    "from keras.layers import TimeDistributed,Conv1D,Dense,Embedding,Input,Dropout,LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate\n",
    "# from prepro import readfile,createBatches,createMatrices,iterate_minibatches,addCharInformatioin,padding\n",
    "from keras.utils import plot_model,Progbar\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "from keras_contrib.layers import CRF\n",
    "from numpy import newaxis\n",
    "import sklearn\n",
    "import subprocess\n",
    "import fastText\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCasing(word, caseLookup):   \n",
    "    casing = 'other'\n",
    "    \n",
    "    numDigits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            numDigits += 1\n",
    "            \n",
    "    digitFraction = numDigits / float(len(word))\n",
    "    \n",
    "    if word.isdigit(): #Is a digit\n",
    "        casing = 'numeric'\n",
    "    elif digitFraction > 0.5:\n",
    "        casing = 'mainly_numeric'\n",
    "    elif word.islower(): #All lower case\n",
    "        casing = 'allLower'\n",
    "    elif word.isupper(): #All upper case\n",
    "        casing = 'allUpper'\n",
    "    elif word[0].isupper(): #is a title, initial char upper, then all lower\n",
    "        casing = 'initialUpper'\n",
    "    elif numDigits > 0:\n",
    "        casing = 'contains_digit'\n",
    "    \n",
    "   \n",
    "    return caseLookup[casing]\n",
    "\n",
    "\n",
    "def createMatrices(sentences, label2Idx, case2Idx,char2Idx):\n",
    "    #{'numeric': 0, 'allLower': 1, 'contains_digit': 6, 'PADDING_TOKEN': 7, 'other': 4, 'allUpper': 2, 'mainly_numeric': 5, 'initialUpper': 3}\n",
    "\n",
    "        \n",
    "    dataset = []\n",
    "    \n",
    "    wordCount = 0\n",
    "    unknownWordCount = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        wordIndices = []\n",
    "        caseIndices = []\n",
    "        charIndices = []\n",
    "        labelIndices = []\n",
    "        \n",
    "        for word,char,label in sentence:  \n",
    "            charIdx = []\n",
    "            for x in char:\n",
    "                if x in char2Idx.keys():\n",
    "                    charIdx.append(char2Idx[x])\n",
    "                else:\n",
    "                    charIdx.append(char2Idx['UNKNOWN'])\n",
    "            #Get the label and map to int            \n",
    "            wordIndices.append(word)\n",
    "            caseIndices.append(getCasing(word, case2Idx))\n",
    "            charIndices.append(charIdx)\n",
    "            labelIndices.append(label2Idx[label])\n",
    "           \n",
    "        dataset.append([wordIndices, caseIndices, charIndices, labelIndices]) \n",
    "        \n",
    "    return dataset\n",
    "\n",
    "def padding(Sentences):\n",
    "    maxlen = 52\n",
    "    for sentence in Sentences:\n",
    "        char = sentence[2]\n",
    "        for x in char:\n",
    "            maxlen = max(maxlen,len(x))\n",
    "    for i,sentence in enumerate(Sentences):\n",
    "        Sentences[i][2] = pad_sequences(Sentences[i][2],52,padding='post')\n",
    "    return Sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_dataset(dataset):\n",
    "    correctLabels = []\n",
    "    predLabels = []\n",
    "    b = Progbar(len(dataset))\n",
    "    for i,data in enumerate(dataset):    \n",
    "        tokens, casing,char, labels = data\n",
    "        tokens = np.asarray([tokens])     \n",
    "        casing = np.asarray([casing])\n",
    "        char = np.asarray([char])\n",
    "        pred = model.predict([tokens, casing,char], verbose=False)[0]   \n",
    "        pred = pred.argmax(axis=-1) #Predict the classes            \n",
    "        correctLabels.append(labels)\n",
    "        predLabels.append(pred)\n",
    "        b.update(i)\n",
    "    return predLabels, correctLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing all deriv and part to misc. with BIO\n",
    "def modify_labels(dataset):\n",
    "    bad_labels = ['I-PERderiv','I-OTHpart','B-ORGderiv', 'I-OTH','B-OTHpart','B-LOCderiv','I-LOCderiv','I-OTHderiv','B-PERderiv','B-OTHderiv','B-PERpart','I-PERpart','I-LOCpart','B-LOCpart','I-ORGpart','I-ORGderiv','B-ORGpart','B-OTH']\n",
    "    for sentence in dataset:\n",
    "        for word in sentence:\n",
    "            label = word[1]\n",
    "            if label in bad_labels:\n",
    "                first_char = label[0]\n",
    "                if first_char == 'B' :\n",
    "                    word[1] = 'B-MISC'\n",
    "                else:\n",
    "                    word[1] = 'I-MISC'\n",
    "    return dataset\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_germeval(path):\n",
    "    sentences = []\n",
    "    with open(path, 'r', encoding = 'UTF-8') as f:\n",
    "        sentence = []\n",
    "        for line in f:\n",
    "            \n",
    "            line = line.strip()\n",
    "            \n",
    "            # append sentence\n",
    "            if len(line) == 0:\n",
    "                if len(sentence):\n",
    "                    sentences.append(sentence)\n",
    "                sentence = []\n",
    "                continue\n",
    "            \n",
    "            # get sentence tokens\n",
    "            splits = line.split()\n",
    "            if splits[0] == '#':\n",
    "                continue\n",
    "            temp = [splits[1],splits[2]]\n",
    "            sentence.append(temp)\n",
    "        \n",
    "        # append last\n",
    "        if len(sentence):\n",
    "            sentences.append(sentence)    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preproecessing data from Conll\n",
    "def get_sentences_conll(filename):\n",
    "    '''\n",
    "        -DOCSTART- -X- -X- O\n",
    "\n",
    "    EU NNP B-NP B-ORG\n",
    "    rejects VBZ B-VP O\n",
    "    German JJ B-NP B-MISC\n",
    "    call NN I-NP O\n",
    "    to TO B-VP O\n",
    "    boycott VB I-VP O\n",
    "    British JJ B-NP B-MISC\n",
    "    lamb NN I-NP O\n",
    "    . . O O\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    read file\n",
    "    return format :\n",
    "    [ ['EU', 'B-ORG'], ['rejects', 'O'], ['German', 'B-MISC'], ['call', 'O'], ['to', 'O'], ['boycott', 'O'], ['British', 'B-MISC'], ['lamb', 'O'], ['.', 'O'] ]\n",
    "    '''\n",
    "    f = open(filename,'rb')\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in f:\n",
    "        splits = line.split()\n",
    "        try:\n",
    "            word=splits[0].decode()\n",
    "            if word=='-DOCSTART-':\n",
    "                continue\n",
    "            label=splits[-1].decode()\n",
    "            temp=[word,label]\n",
    "            sentence.append(temp)\n",
    "        except Exception as e:\n",
    "            if len(sentence)!=0:\n",
    "                sentences.append(sentence)\n",
    "                sentence=[]\n",
    "    return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000\n",
      "2200\n",
      "5100\n"
     ]
    }
   ],
   "source": [
    "trainSentences = get_sentences_germeval('../data/GermEVAL/NER-de-train.tsv')\n",
    "devSentences = get_sentences_germeval('../data/GermEVAL/NER-de-dev.tsv')\n",
    "testSentences = get_sentences_germeval('../data/GermEVAL/NER-de-test.tsv')\n",
    "\n",
    "# trainSentences = get_sentences('../data/CONLL/deu/deu_utf.train')\n",
    "# devSentences = get_sentences('../data/CONLL/deu/deu_utf.testa')\n",
    "# testSentences = get_sentences('../data/CONLL/deu/deu_utf.testb')\n",
    "\n",
    "print(len(trainSentences))\n",
    "print(len(devSentences))\n",
    "print(len(testSentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1951', 'O'], ['bis', 'O'], ['1953', 'O'], ['wurde', 'O'], ['der', 'O'], ['nördliche', 'O'], ['Teil', 'O'], ['als', 'O'], ['Jugendburg', 'O'], ['des', 'O'], ['Kolpingwerkes', 'B-OTH'], ['gebaut', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(testSentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelSet = set()\n",
    "characters= set()\n",
    "\n",
    "for dataset in [trainSentences, devSentences, testSentences]:\n",
    "    for sentence in dataset:\n",
    "        for word, label in sentence:\n",
    "            for char in word:\n",
    "                characters.add(char)\n",
    "            labelSet.add(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "print(len(labelSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Create a mapping for the labels ::\n",
    "label2Idx = {}\n",
    "for label in labelSet:\n",
    "    label2Idx[label] = len(label2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-PERpart': 0, 'B-LOCderiv': 16, 'B-PER': 8, 'B-OTHderiv': 9, 'I-OTHpart': 10, 'I-OTH': 11, 'I-OTHderiv': 12, 'B-PERderiv': 13, 'I-PERderiv': 14, 'B-LOCpart': 1, 'I-ORGpart': 2, 'B-LOC': 15, 'B-OTHpart': 3, 'I-ORGderiv': 18, 'B-ORGpart': 4, 'B-ORGderiv': 19, 'B-ORG': 20, 'O': 5, 'I-ORG': 6, 'I-PER': 17, 'I-LOC': 21, 'B-OTH': 22, 'I-LOCpart': 23, 'I-PERpart': 7, 'I-LOCderiv': 24}\n"
     ]
    }
   ],
   "source": [
    "print(label2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Hard coded case lookup ::\n",
    "case2Idx = {'numeric': 0, 'allLower':1, 'allUpper':2, 'initialUpper':3, 'other':4, 'mainly_numeric':5, 'contains_digit': 6, 'PADDING_TOKEN':7}\n",
    "caseEmbeddings = np.identity(len(case2Idx), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "{'other': 4, 'contains_digit': 6, 'allUpper': 2, 'mainly_numeric': 5, 'allLower': 1, 'PADDING_TOKEN': 7, 'numeric': 0, 'initialUpper': 3}\n"
     ]
    }
   ],
   "source": [
    "print(caseEmbeddings)\n",
    "print(case2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(trainSentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Schartau', 'B-PER'], ['sagte', 'O'], ['dem', 'O'], ['\"', 'O'], ['Tagesspiegel', 'B-ORG'], ['\"', 'O'], ['vom', 'O'], ['Freitag', 'O'], [',', 'O'], ['Fischer', 'B-PER'], ['sei', 'O'], ['\"', 'O'], ['in', 'O'], ['einer', 'O'], ['Weise', 'O'], ['aufgetreten', 'O'], [',', 'O'], ['die', 'O'], ['alles', 'O'], ['andere', 'O'], ['als', 'O'], ['überzeugend', 'O'], ['war', 'O'], ['\"', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(trainSentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'R': 106, '1': 0, 'Е': 240, '~': 2, 'ъ': 313, 'ế': 3, '¹': 4, '£': 5, 'I': 6, 'ý': 7, 'î': 8, 'Á': 9, '?': 10, 'П': 279, '³': 12, '佐': 268, '鷹': 13, 'κ': 14, 'Л': 63, '南': 15, 'ô': 16, '‘': 17, 'İ': 19, 'ž': 35, '_': 22, 'Ġ': 293, 'С': 222, 'ā': 23, 'ç': 1, '’': 24, '€': 25, 'j': 28, 'з': 29, 'ó': 30, '≤': 31, 'Ż': 32, 'G': 247, '`': 33, 'ħ': 11, 'Ş': 38, '별': 40, 'š': 39, '\\x94': 41, 'ō': 42, '#': 218, 'ń': 43, '°': 217, '公': 44, 'υ': 45, '⊃': 46, 'ب': 47, 'i': 48, 'à': 51, '}': 50, 'н': 52, '×': 53, 'ö': 55, '章': 56, '‐': 57, 'û': 58, 'M': 59, 'ś': 62, ':': 281, '\\u200e': 114, 'ú': 64, 'р': 65, '–': 286, 'ŏ': 296, 'k': 67, 'ę': 181, 'ē': 70, '守': 75, 'a': 71, 'P': 68, 'ـ': 74, 'd': 76, 'τ': 69, 'ū': 78, 'М': 282, 'д': 80, '太': 321, 'ê': 82, 'á': 256, 'ε': 117, 'È': 90, 'ř': 73, 'z': 86, 'í': 177, '$': 314, 'ḳ': 91, 'L': 180, 'Q': 92, '算': 310, 'D': 93, 'ς': 94, ']': 95, '대': 96, 'λ': 97, '台': 99, '%': 87, 'ĩ': 100, 'ë': 101, 'В': 102, 'ρ': 104, 'c': 124, 'о': 105, '<': 18, 'е': 108, 'O': 107, '²': 77, '李': 109, '→': 121, 'ψ': 110, '«': 111, '傳': 295, 'γ': 128, 'в': 112, 'ø': 113, 'б': 115, 'ņ': 116, 'к': 231, 'f': 72, '″': 118, '鶴': 123, 'т': 119, 'ж': 120, ')': 298, 'l': 122, 'Š': 81, 'м': 125, '⋅': 21, '!': 126, 'β': 132, 'ν': 129, 'и': 131, 'õ': 83, 'ź': 137, '造': 136, 'ť': 84, '@': 312, '=': 284, 'ệ': 139, 'ن': 146, 'φ': 141, 'ά': 133, '\\x92': 142, 'ا': 144, 'C': 145, '—': 79, '殿': 147, 'Ш': 243, '3': 149, 'у': 151, 'ο': 154, 'r': 152, '寝': 135, '東': 158, '▪': 156, '*': 157, '士': 26, 'Â': 159, 'J': 160, 't': 89, '博': 164, 'w': 163, 'X': 165, 'ü': 166, '학': 168, 'A': 27, 'x': 169, 's': 170, '樓': 290, 'ä': 171, 'Y': 299, '≘': 174, 'ы': 138, '\\x9a': 258, ';': 251, 'ɨ': 176, 'é': 20, 'ě': 188, '5': 178, '\\xad': 283, '”': 179, 'ğ': 140, 'q': 182, 'ḫ': 183, 'å': 306, 'ă': 185, 'Ö': 186, 'o': 88, '루': 189, '±': 190, 'ι': 134, 'ň': 276, 'х': 191, 'Œ': 192, 'ş': 193, 'ь': 194, '¸': 195, 'Ł': 197, 'K': 198, 'η': 202, 'а': 200, 'ό': 36, 'e': 203, 'W': 205, 'π': 252, 'я': 66, 'V': 34, 'ī': 173, '§': 201, '−': 211, 'E': 212, '\\x99': 37, 'Å': 214, 'ő': 322, 'u': 215, '\"': 216, 'ю': 196, 'л': 210, 'B': 220, '“': 221, 'Т': 223, '‹': 224, '9': 150, 'Č': 226, '>': 327, 'Î': 228, 'À': 229, 'â': 230, '†': 153, 'έ': 219, 'n': 235, '柯': 236, '&': 249, '„': 237, '´': 238, 'с': 239, 'オ': 241, '›': 242, 'Ü': 315, '2': 148, 'ї': 244, '妃': 289, '8': 187, 'ラ': 250, '0': 199, 'ð': 254, 'h': 302, '+': 232, 'É': 175, '/': 253, 'ż': 225, 'У': 255, '4': 155, 'ı': 257, 'Π': 98, 'H': 259, 'g': 260, 'Λ': 262, 'Ц': 261, ',': 264, '7': 266, 'σ': 267, 'Þ': 204, 'æ': 270, '(': 172, '\\x96': 272, 'Ä': 273, 'UNKNOWN': 328, '·': 274, 'й': 275, '[': 263, 'ą': 277, '…': 278, 'α': 297, 'ć': 61, 'S': 280, '동': 184, 'أ': 288, \"'\": 209, 'ã': 285, 'ʻ': 265, 'г': 287, 'И': 206, '貴': 291, 'Ø': 308, 'Ž': 49, 'T': 130, 'ǒ': 292, '©': 294, 'ł': 103, 'F': 208, 'U': 248, 'Æ': 161, '懿': 269, '九': 300, 'ċ': 227, 'y': 162, '.': 319, '½': 318, '別': 301, '\\x95': 234, '‚': 233, 'p': 303, 'ß': 304, 'ῦ': 54, 'µ': 307, '¤': 309, 'č': 325, '術': 143, '6': 311, '\\x80': 213, 'b': 245, 'п': 60, 'Z': 305, '冲': 316, 'è': 317, 'ñ': 127, '»': 271, 'є': 320, 'm': 323, 'ي': 324, '-': 246, 'v': 207, 'ض': 167, 'œ': 326, 'N': 85}\n"
     ]
    }
   ],
   "source": [
    "char2Idx={}\n",
    "for char in characters:\n",
    "    char2Idx[char] = len(char2Idx)\n",
    "char2Idx['UNKNOWN'] = len(char2Idx)\n",
    "print(char2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Schartau', 'B-PER'], ['sagte', 'O'], ['dem', 'O'], ['\"', 'O'], ['Tagesspiegel', 'B-ORG'], ['\"', 'O'], ['vom', 'O'], ['Freitag', 'O'], [',', 'O'], ['Fischer', 'B-PER'], ['sei', 'O'], ['\"', 'O'], ['in', 'O'], ['einer', 'O'], ['Weise', 'O'], ['aufgetreten', 'O'], [',', 'O'], ['die', 'O'], ['alles', 'O'], ['andere', 'O'], ['als', 'O'], ['überzeugend', 'O'], ['war', 'O'], ['\"', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(trainSentences[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fastText.load_model(\"../embeddings/wiki.de.bin\")\n",
    "# ft = fastText.load_model(\"../embeddings/cc.de.300.bin\")\n",
    "\n",
    "nb_embedding_dims = ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "print(nb_embedding_dims)\n",
    "print(len(trainSentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Schartau', 'B-PER'], ['sagte', 'O'], ['dem', 'O'], ['\"', 'O'], ['Tagesspiegel', 'B-ORG'], ['\"', 'O'], ['vom', 'O'], ['Freitag', 'O'], [',', 'O'], ['Fischer', 'B-PER'], ['sei', 'O'], ['\"', 'O'], ['in', 'O'], ['einer', 'O'], ['Weise', 'O'], ['aufgetreten', 'O'], [',', 'O'], ['die', 'O'], ['alles', 'O'], ['andere', 'O'], ['als', 'O'], ['überzeugend', 'O'], ['war', 'O'], ['\"', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(trainSentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBatches(dataset):\n",
    "    l = []\n",
    "    for i in dataset:\n",
    "        l.append(len(i))\n",
    "    l = set(l)\n",
    "    print(len(l))\n",
    "    batches = []\n",
    "    batch_len = []\n",
    "    z = 0\n",
    "    for i in l:\n",
    "        temp = []\n",
    "        for batch in dataset:\n",
    "            if len(batch) == i:\n",
    "                temp.append(batch)\n",
    "                z += 1\n",
    "        batches.append(temp)\n",
    "#         batch_len.append(z)\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "45\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "train_batches = createBatches(trainSentences)\n",
    "dev_batches = createBatches(devSentences)\n",
    "test_batches = createBatches(testSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "3\n",
      "[[['Alles', 'O'], ['richtig', 'O'], ['.', 'O']], [['Farben', 'O'], ['eingeführt', 'O'], ['.', 'O']], [['Material', 'O'], ['gewinnen', 'O'], ['.', 'O']]]\n"
     ]
    }
   ],
   "source": [
    "batches = train_batches\n",
    "print(len(batches))\n",
    "print(len(batches[0]))\n",
    "print(batches[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(batches: 'list of training/dev sentences- batches already created'):\n",
    "    global line_number\n",
    "    \n",
    "    while True:\n",
    "        for batch in batches:\n",
    "            word_embeddings = []\n",
    "            case_embeddings = []\n",
    "            char_embeddings = []\n",
    "\n",
    "            output_labels = []\n",
    "            for index in range(len(batch)): # batches made according to the size of the sentences. len(batch) gives the size of current batch\n",
    "                sentence = batch[index]\n",
    "    #             print(sentence)\n",
    "                temp_casing = []\n",
    "                temp_char=[]\n",
    "                temp_word=[]\n",
    "                temp_output=[]\n",
    "                for word in sentence:\n",
    "                    word, label = word\n",
    "                    casing =getCasing(word, case2Idx)\n",
    "                    temp_casing.append(casing)\n",
    "                    temp_char2=[]\n",
    "                    for char in word:\n",
    "                        if char in char2Idx.keys():\n",
    "                            temp_char2.append(char2Idx[char])\n",
    "                        else:\n",
    "                            temp_char2.append(char2Idx['UNKNOWN']) # To incorporate the words which are not in the vocab\n",
    "                    temp_char2 = np.array(temp_char2)\n",
    "                    temp_char.append(temp_char2)\n",
    "                    word_vector = ft.get_word_vector(word.lower())\n",
    "                    # word_vector = ft.get_word_vector(word)\n",
    "                    temp_word.append(word_vector)\n",
    "                    temp_output.append(label2Idx[label])\n",
    "                temp_char = pad_sequences(temp_char, 52)\n",
    "                word_embeddings.append(temp_word)\n",
    "                case_embeddings.append(temp_casing)\n",
    "                char_embeddings.append(temp_char)\n",
    "                temp_output = to_categorical(temp_output, 25)\n",
    "                output_labels.append(temp_output)\n",
    "    #             output_labels = to_categorical()\n",
    "    #             output_labels = np.array(output_labels)\n",
    "    #             output_labels = output_labels[...,newaxis]\n",
    "\n",
    "    #             print(np.array(word_embeddings).shape)\n",
    "    #             print(np.array(case_embeddings).shape)\n",
    "    #             print(np.array(char_embeddings).shape)\n",
    "    #             print(output_labels.shape)\n",
    "    #             print(\"******************\\n\\n\")\n",
    "            yield ([np.array(word_embeddings), np.array(case_embeddings), np.array(char_embeddings)], np.array(output_labels))\n",
    "\n",
    "def get_label_from_categorical(a):\n",
    "    labels = []\n",
    "    for label in a:\n",
    "        label = np.ndarray.tolist(label)\n",
    "        label = np.argmax(label)\n",
    "        labels.append(label)\n",
    "    return(labels)\n",
    "\n",
    "def predict_batches(batch):\n",
    "    steps = 0\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for input_data, output_data in generator(batch):\n",
    "        pred_labels_batch = model.predict(input_data)\n",
    "        for s in pred_labels_batch:\n",
    "            pred_labels.append(get_label_from_categorical(s))\n",
    "        for s in output_data:\n",
    "            true_labels.append(get_label_from_categorical(s))\n",
    "        steps += 1\n",
    "        if steps == len(batch):\n",
    "            break\n",
    "    return(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2Label = {v: k for k, v in label2Idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for s in y_pred:\n",
    "        pred_labels.append(get_label_from_categorical(s))\n",
    "    for s in y_true:\n",
    "        true_labels.append(get_label_from_categorical(s))\n",
    "    p, r, f = compute_f1(y_pred, y_true, idx2Label)\n",
    "    return r\n",
    "\n",
    "def get_model():\n",
    "    words_input = Input(shape=(None, nb_embedding_dims), dtype='float32', name='words_input')\n",
    "    casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
    "    casing = Embedding(output_dim=caseEmbeddings.shape[1], input_dim=caseEmbeddings.shape[0], weights=[caseEmbeddings], trainable=False, name = 'case_embed')(casing_input)\n",
    "    character_input=Input(shape=(None,52,),name='char_input')\n",
    "    embed_char_out=TimeDistributed(Embedding(len(char2Idx),32,embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input)\n",
    "    kernel_sizes = (3, 4, 5)\n",
    "    conv_blocks = []\n",
    "    for sz in kernel_sizes:\n",
    "        conv = TimeDistributed(Conv1D(\n",
    "                             kernel_size=sz,\n",
    "                             filters=32,\n",
    "                             padding=\"same\",\n",
    "                             activation=\"relu\",\n",
    "                             strides=1))(embed_char_out)\n",
    "        conv = TimeDistributed(MaxPooling1D(52))(conv)\n",
    "        conv = TimeDistributed(Flatten())(conv)\n",
    "        conv_blocks.append(conv)\n",
    "    output = concatenate([words_input, casing, conv_blocks[0], conv_blocks[1], conv_blocks[2]])\n",
    "    output = Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.5))(output)\n",
    "    output = TimeDistributed(Dense(len(label2Idx)))(output)\n",
    "    crf = CRF(len(label2Idx))\n",
    "    output = crf(output)\n",
    "    model = Model(inputs=[words_input, casing_input, character_input], outputs=[output])\n",
    "    model.compile(loss=crf.loss_function, optimizer='nadam', metrics=[crf.accuracy])\n",
    "    model.summary()\n",
    "    return(model)\n",
    "\n",
    "class F1History(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.acc = []\n",
    "        self.f1_scores = []\n",
    "        self.max_f1 = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.acc.append(logs.get('val_acc'))\n",
    "        true_labels, pred_labels = predict_batches(dev_batches)\n",
    "        pre, rec, f1 = compute_f1(pred_labels, true_labels, idx2Label)\n",
    "        self.f1_scores.append(f1)\n",
    "        if epoch > 30 and f1 > self.max_f1:\n",
    "            print(\"\\nNew maximum F1 score: \" + str(f1) + \" (before: \" + str(self.max_f1) + \") Saving to \" + tmp_model_filename)\n",
    "            self.max_f1 = f1\n",
    "            model.save(tmp_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, None, 52)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, None, 52, 32) 10528       char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_11 (TimeDistri (None, None, 52, 32) 3104        char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_14 (TimeDistri (None, None, 52, 32) 4128        char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_17 (TimeDistri (None, None, 52, 32) 5152        char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "casing_input (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_12 (TimeDistri (None, None, 1, 32)  0           time_distributed_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_15 (TimeDistri (None, None, 1, 32)  0           time_distributed_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_18 (TimeDistri (None, None, 1, 32)  0           time_distributed_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "words_input (InputLayer)        (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "case_embed (Embedding)          (None, None, 8)      64          casing_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_13 (TimeDistri (None, None, 32)     0           time_distributed_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_16 (TimeDistri (None, None, 32)     0           time_distributed_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_19 (TimeDistri (None, None, 32)     0           time_distributed_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, 404)    0           words_input[0][0]                \n",
      "                                                                 case_embed[0][0]                 \n",
      "                                                                 time_distributed_13[0][0]        \n",
      "                                                                 time_distributed_16[0][0]        \n",
      "                                                                 time_distributed_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, None, 400)    968000      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_20 (TimeDistri (None, None, 25)     10025       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "crf_2 (CRF)                     (None, None, 25)     1325        time_distributed_20[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 1,002,326\n",
      "Trainable params: 1,002,262\n",
      "Non-trainable params: 64\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/80\n",
      "51/51 [==============================] - 46s 907ms/step - loss: 0.5811 - acc: 0.8812 - val_loss: 0.9375 - val_acc: 0.6589\n",
      "Epoch 2/80\n",
      "51/51 [==============================] - 28s 549ms/step - loss: 0.3196 - acc: 0.9232 - val_loss: 1.4223 - val_acc: 0.2033\n",
      "Epoch 3/80\n",
      "51/51 [==============================] - 29s 564ms/step - loss: 0.2602 - acc: 0.9342 - val_loss: 1.3636 - val_acc: 0.2428\n",
      "Epoch 4/80\n",
      "51/51 [==============================] - 28s 540ms/step - loss: 0.2254 - acc: 0.9375 - val_loss: 0.4643 - val_acc: 0.8165\n",
      "Epoch 5/80\n",
      "51/51 [==============================] - 28s 546ms/step - loss: 0.1864 - acc: 0.9442 - val_loss: 0.3915 - val_acc: 0.8489\n",
      "Epoch 6/80\n",
      "50/51 [============================>.] - ETA: 0s - loss: 0.1529 - acc: 0.9517"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/kerasenv/lib/python3.5/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'argmax'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-4316ae03d737>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;32m~/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2211\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2213\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2214\u001b[0m                 \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2215\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-88144c9e1877>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mtrue_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mpre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_f1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx2Label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-2a9a19a66ebe>\u001b[0m in \u001b[0;36mpredict_batches\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mpred_labels_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_labels_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mpred_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_label_from_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mtrue_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_label_from_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-2a9a19a66ebe>\u001b[0m in \u001b[0;36mget_label_from_categorical\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kerasenv/lib/python3.5/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m     \"\"\"\n\u001b[0;32m-> 1004\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kerasenv/lib/python3.5/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# a downstream library like 'pandas'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kerasenv/lib/python3.5/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tmp_model_filename = 'tmp_generator_NER_best.h5'\n",
    "# checkpoint = ModelCheckpoint(tmp_model_filename, verbose=1, save_best_only = True, monitor = 'val_acc')\n",
    "history = F1History()\n",
    "model = get_model()\n",
    "model.fit_generator(\n",
    "    generator(train_batches), \n",
    "    epochs = 80, steps_per_epoch = len(train_batches), \n",
    "    validation_data = generator(dev_batches), validation_steps = len(dev_batches), \n",
    "    callbacks = [history]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.acc)\n",
    "print(history.f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(tmp_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels, pred_labels = predict_batches(test_batches)\n",
    "print(compute_f1(pred_labels, true_labels, idx2Label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('results_wiki.txt', 'w')\n",
    "for run_i in range(13):\n",
    "    print(\"Run \" + str(run_i))\n",
    "    \n",
    "    tmp_model_filename = 'tmp_generator_NER_best.' + str(run_i) + '.h5'\n",
    "    # tmp_model_filename = 'tmp_generator_NER_best.h5'\n",
    "    # checkpoint = ModelCheckpoint(tmp_model_filename, verbose=1, save_best_only = True, monitor = 'val_acc')\n",
    "    history = F1History()\n",
    "    \n",
    "    model = get_model()\n",
    "    model.fit_generator(\n",
    "        generator(train_batches), \n",
    "        epochs = 80, steps_per_epoch = len(train_batches), \n",
    "        validation_data = generator(dev_batches), validation_steps = len(dev_batches), \n",
    "        callbacks = [history]\n",
    "    )\n",
    "    \n",
    "    model.load_weights(tmp_model_filename)\n",
    "    true_labels, pred_labels = predict_batches(test_batches)\n",
    "    \n",
    "    pre, rec, f1 = compute_f1(pred_labels, true_labels, idx2Label)\n",
    "    f.write(str(run_i) + \"\\t\" + str(pre) + \"\\t\" + str(rec) +  \"\\t\" + str(f1))\n",
    "    f.write(\"\\n\")\n",
    "    f.flush()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, json\n",
    "# copy file for best run\n",
    "shutil.copyfile('tmp_generator_NER_best.0.h5', 'final_model_germeval.h5')\n",
    "with open(\"final_model_germeval.indexes\", \"w\") as f:\n",
    "    json.dump([idx2Label, label2Idx, char2Idx, case2Idx], f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
