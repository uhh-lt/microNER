{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np \n",
    "from keras.utils import to_categorical\n",
    "from validation import compute_f1\n",
    "from keras.models import Model\n",
    "from keras.layers import TimeDistributed,Conv1D,Dense,Embedding,Input,Dropout,LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate\n",
    "# from prepro import readfile,createBatches,createMatrices,iterate_minibatches,addCharInformatioin,padding\n",
    "from keras.utils import plot_model,Progbar\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "from keras_contrib.layers import CRF\n",
    "from numpy import newaxis\n",
    "import sklearn\n",
    "import subprocess\n",
    "import fastText\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCasing(word, caseLookup):   \n",
    "    casing = 'other'\n",
    "    \n",
    "    numDigits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            numDigits += 1\n",
    "            \n",
    "    digitFraction = numDigits / float(len(word))\n",
    "    \n",
    "    if word.isdigit(): #Is a digit\n",
    "        casing = 'numeric'\n",
    "    elif digitFraction > 0.5:\n",
    "        casing = 'mainly_numeric'\n",
    "    elif word.islower(): #All lower case\n",
    "        casing = 'allLower'\n",
    "    elif word.isupper(): #All upper case\n",
    "        casing = 'allUpper'\n",
    "    elif word[0].isupper(): #is a title, initial char upper, then all lower\n",
    "        casing = 'initialUpper'\n",
    "    elif numDigits > 0:\n",
    "        casing = 'contains_digit'\n",
    "    return caseLookup[casing]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing all deriv and part to misc. with BIO\n",
    "def modify_labels(dataset):\n",
    "    bad_labels = ['I-PERderiv','I-OTHpart','B-ORGderiv', 'I-OTH','B-OTHpart','B-LOCderiv','I-LOCderiv','I-OTHderiv','B-PERderiv','B-OTHderiv','B-PERpart','I-PERpart','I-LOCpart','B-LOCpart','I-ORGpart','I-ORGderiv','B-ORGpart','B-OTH']\n",
    "    for sentence in dataset:\n",
    "        for word in sentence:\n",
    "            label = word[1]\n",
    "            if label in bad_labels:\n",
    "                first_char = label[0]\n",
    "                if first_char == 'B' :\n",
    "                    word[1] = 'B-MISC'\n",
    "                else:\n",
    "                    word[1] = 'I-MISC'\n",
    "    return dataset\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_germeval(path):\n",
    "    sentences = []\n",
    "    with open(path, 'r', encoding = 'UTF-8') as f:\n",
    "        sentence = []\n",
    "        for line in f:\n",
    "            \n",
    "            line = line.strip()\n",
    "            \n",
    "            # append sentence\n",
    "            if len(line) == 0:\n",
    "                if len(sentence):\n",
    "                    sentences.append(sentence)\n",
    "                sentence = []\n",
    "                continue\n",
    "            \n",
    "            # get sentence tokens\n",
    "            splits = line.split()\n",
    "            if splits[0] == '#':\n",
    "                continue\n",
    "            temp = [splits[1], splits[3], splits[2]]\n",
    "            sentence.append(temp)\n",
    "        \n",
    "        # append last\n",
    "        if len(sentence):\n",
    "            sentences.append(sentence)    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preproecessing data from Conll\n",
    "def get_sentences_conll(filename):\n",
    "    '''\n",
    "        -DOCSTART- -X- -X- O\n",
    "\n",
    "    EU NNP B-NP B-ORG\n",
    "    rejects VBZ B-VP O\n",
    "    German JJ B-NP B-MISC\n",
    "    call NN I-NP O\n",
    "    to TO B-VP O\n",
    "    boycott VB I-VP O\n",
    "    British JJ B-NP B-MISC\n",
    "    lamb NN I-NP O\n",
    "    . . O O\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    read file\n",
    "    return format :\n",
    "    [ ['EU', 'B-ORG'], ['rejects', 'O'], ['German', 'B-MISC'], ['call', 'O'], ['to', 'O'], ['boycott', 'O'], ['British', 'B-MISC'], ['lamb', 'O'], ['.', 'O'] ]\n",
    "    '''\n",
    "    f = open(filename,'rb')\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in f:\n",
    "        splits = line.split()\n",
    "        try:\n",
    "            word=splits[0].decode()\n",
    "            if word=='-DOCSTART-':\n",
    "                continue\n",
    "            label=splits[-1].decode()\n",
    "            temp=[word,label]\n",
    "            sentence.append(temp)\n",
    "        except Exception as e:\n",
    "            if len(sentence)!=0:\n",
    "                sentences.append(sentence)\n",
    "                sentence=[]\n",
    "    return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000\n",
      "2200\n",
      "5100\n"
     ]
    }
   ],
   "source": [
    "trainSentences = get_sentences_germeval('../data/GermEVAL/NER-de-train.tsv')\n",
    "devSentences = get_sentences_germeval('../data/GermEVAL/NER-de-dev.tsv')\n",
    "testSentences = get_sentences_germeval('../data/GermEVAL/NER-de-test.tsv')\n",
    "\n",
    "# trainSentences = get_sentences('../data/CONLL/deu/deu_utf.train')\n",
    "# devSentences = get_sentences('../data/CONLL/deu/deu_utf.testa')\n",
    "# testSentences = get_sentences('../data/CONLL/deu/deu_utf.testb')\n",
    "\n",
    "print(len(trainSentences))\n",
    "print(len(devSentences))\n",
    "print(len(testSentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1951', 'O', 'O'], ['bis', 'O', 'O'], ['1953', 'O', 'O'], ['wurde', 'O', 'O'], ['der', 'O', 'O'], ['nördliche', 'O', 'O'], ['Teil', 'O', 'O'], ['als', 'O', 'O'], ['Jugendburg', 'O', 'O'], ['des', 'O', 'O'], ['Kolpingwerkes', 'O', 'B-OTH'], ['gebaut', 'O', 'O'], ['.', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(testSentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelSet = set()\n",
    "characters= set()\n",
    "labelL1Set = set()\n",
    "\n",
    "for dataset in [trainSentences, devSentences, testSentences]:\n",
    "    for sentence in dataset:\n",
    "        for word, label, L1 in sentence:\n",
    "            for char in word:\n",
    "                characters.add(char)\n",
    "            labelSet.add(label)\n",
    "            labelL1Set.add(L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(labelSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Create a mapping for the labels ::\n",
    "label2Idx = {}\n",
    "for label in labelSet:\n",
    "    label2Idx[label] = len(label2Idx)\n",
    "    \n",
    "labelL12Idx = {}\n",
    "for label in labelL1Set:\n",
    "    labelL12Idx[label] = len(labelL12Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B-ORGpart': 2, 'B-LOCderiv': 14, 'I-ORG': 3, 'B-PER': 4, 'B-ORG': 5, 'B-ORGderiv': 6, 'B-OTHderiv': 7, 'B-OTH': 8, 'B-PERderiv': 9, 'B-OTHpart': 10, 'B-LOCpart': 1, 'I-OTH': 11, 'I-PER': 12, 'I-LOC': 13, 'B-LOC': 16, 'I-LOCderiv': 17, 'B-PERpart': 15}\n",
      "{'B-LOCpart': 11, 'B-ORGpart': 0, 'B-OTHpart': 12, 'I-OTH': 1, 'I-PER': 13, 'I-LOC': 14, 'I-OTHpart': 10, 'I-ORGpart': 2, 'O': 15, 'I-OTHderiv': 3, 'I-ORG': 16, 'B-PER': 4, 'I-PERpart': 17, 'B-ORG': 18, 'B-ORGderiv': 19, 'B-OTHderiv': 5, 'B-OTH': 20, 'B-PERderiv': 6, 'I-PERderiv': 21, 'I-ORGderiv': 7, 'B-PERpart': 8, 'B-LOCderiv': 22, 'B-LOC': 9, 'I-LOCderiv': 23, 'I-LOCpart': 24}\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1.]]\n"
     ]
    }
   ],
   "source": [
    "print(label2Idx)\n",
    "print(labelL12Idx)\n",
    "L1embeddings = np.identity(len(labelL12Idx), dtype='float32')\n",
    "print(L1embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Hard coded case lookup ::\n",
    "case2Idx = {'numeric': 0, 'allLower':1, 'allUpper':2, 'initialUpper':3, 'other':4, 'mainly_numeric':5, 'contains_digit': 6, 'PADDING_TOKEN':7}\n",
    "caseEmbeddings = np.identity(len(case2Idx), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "{'other': 4, 'PADDING_TOKEN': 7, 'contains_digit': 6, 'allLower': 1, 'initialUpper': 3, 'numeric': 0, 'allUpper': 2, 'mainly_numeric': 5}\n"
     ]
    }
   ],
   "source": [
    "print(caseEmbeddings)\n",
    "print(case2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(trainSentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Schartau', 'O'], ['sagte', 'O'], ['dem', 'O'], ['\"', 'O'], ['Tagesspiegel', 'O'], ['\"', 'O'], ['vom', 'O'], ['Freitag', 'O'], [',', 'O'], ['Fischer', 'O'], ['sei', 'O'], ['\"', 'O'], ['in', 'O'], ['einer', 'O'], ['Weise', 'O'], ['aufgetreten', 'O'], [',', 'O'], ['die', 'O'], ['alles', 'O'], ['andere', 'O'], ['als', 'O'], ['überzeugend', 'O'], ['war', 'O'], ['\"', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(trainSentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ʻ': 0, 'Æ': 1, '冲': 256, 'p': 2, '›': 276, 'Â': 274, '[': 3, 'У': 4, 'أ': 216, '7': 6, 'Á': 7, 'з': 8, 'Î': 327, 'т': 9, '≘': 271, 'õ': 12, 'ť': 104, '~': 13, 'ń': 15, 'ő': 17, '0': 16, 'λ': 108, 'Л': 19, 'ế': 20, '<': 21, 'Ş': 242, 'ą': 22, '/': 23, 'H': 25, 'й': 26, '柯': 55, '¸': 27, 'έ': 32, '½': 31, 'e': 30, 'v': 33, 'а': 35, 'ا': 36, 'F': 37, '\\x95': 38, '“': 39, '(': 325, '\\x92': 41, '±': 42, 'X': 43, 'о': 308, '‘': 226, '#': 45, 'б': 116, 'w': 46, '鷹': 5, '=': 263, 'オ': 47, 'G': 277, 'Ш': 48, 'ē': 49, 'г': 212, 'κ': 50, 'ā': 52, 'I': 53, 'q': 54, '太': 56, 'ῦ': 57, '⊃': 285, '+': 58, 'ř': 60, 'ς': 62, 'S': 61, '-': 117, 'М': 282, '守': 65, '´': 64, '傳': 70, 'ę': 67, '–': 68, 'ラ': 69, '章': 71, 'Ġ': 273, '별': 73, '\\x94': 74, 'л': 75, 'g': 76, '—': 324, 'ø': 230, 'á': 281, 'n': 77, 'є': 78, \"'\": 79, 'Ö': 80, '九': 292, 'ρ': 82, 'İ': 83, '9': 259, '\\u200e': 10, 'ъ': 85, 'ú': 290, '‚': 86, '°': 87, '佐': 126, '別': 93, 'c': 90, 'В': 91, 'Т': 11, '南': 94, 'ň': 95, 'Q': 63, '>': 101, '`': 99, 'ε': 100, '¹': 235, 'ý': 102, '@': 103, 'Ø': 209, 'z': 14, 'û': 106, '鶴': 18, 'h': 107, 'u': 66, 'ī': 122, '†': 110, '!': 114, 'U': 112, 'r': 113, '$': 105, '„': 118, 'ي': 119, 'ê': 238, '동': 120, '\\x96': 121, 'ż': 298, '術': 123, '造': 125, 'o': 127, 'φ': 128, 'α': 129, 'Λ': 130, 'ь': 131, 'b': 132, '6': 133, 'ḳ': 134, 'J': 135, 'у': 51, 'ο': 136, '″': 137, 'ç': 72, '?': 138, 'ы': 140, '8': 142, 'η': 146, '대': 147, 'ċ': 265, 'ї': 152, 'ü': 149, '*': 153, 'ά': 253, 'm': 154, '.': 148, 'ŏ': 158, 'ب': 157, 'ă': 24, '\\x80': 159, 'ı': 160, '·': 161, '台': 162, '⋅': 291, '1': 302, ']': 188, 'д': 163, 'A': 164, '3': 165, 'ņ': 166, 'æ': 167, 'ã': 168, '懿': 169, 'å': 244, 'ό': 170, '▪': 171, '≤': 246, 'B': 172, '}': 173, '博': 174, 'Ä': 175, 'i': 176, '\\x9a': 177, '算': 178, '樓': 179, '殿': 180, '루': 181, 'ō': 182, '\\xad': 248, 'À': 183, 'š': 28, 'ю': 184, '©': 185, 'ū': 186, 'Š': 187, '%': 29, '東': 189, 'è': 190, 'ö': 191, 'Œ': 192, 'Ž': 193, '‹': 194, 'γ': 251, '²': 195, '×': 196, 'с': 197, 'С': 198, '’': 199, 'ệ': 200, 'ĩ': 310, 'п': 81, '»': 201, 'à': 203, 'ó': 139, 'И': 304, 'â': 204, 'V': 115, 'D': 205, 'П': 207, 'd': 208, 'ж': 227, 'ł': 144, 's': 210, 'N': 211, '士': 215, 'L': 213, 'σ': 214, '貴': 218, 'ź': 217, '_': 219, '£': 220, 'j': 143, 'ν': 222, '³': 223, '학': 224, 'Z': 225, 'ض': 145, 't': 228, ';': 229, 'и': 84, 'Е': 232, 'ð': 233, '”': 234, 'y': 236, '«': 237, 'µ': 40, 'Ż': 239, 'р': 240, '寝': 241, 'É': 231, 'ô': 243, 'R': 150, 'ψ': 221, 'ë': 151, 'x': 247, 'я': 249, 'к': 250, 'υ': 254, '…': 252, 'Č': 255, 'î': 124, '§': 92, '¤': 245, '\\x99': 257, 'M': 88, ')': 258, 'ɨ': 313, 'Ц': 89, 'ş': 202, 'ħ': 267, ',': 266, 'í': 268, 'Y': 269, 'ñ': 272, '公': 275, 'UNKNOWN': 328, 'T': 260, '5': 155, 'Ü': 279, 'f': 44, '\"': 280, 'е': 156, 'ـ': 109, 'č': 262, 'β': 284, '妃': 321, 'τ': 286, '−': 287, 'в': 288, 'E': 289, 'ž': 301, 'Ł': 293, 'ä': 306, 'P': 96, 'х': 295, 'C': 296, 'œ': 297, 'l': 97, 'Þ': 299, 'ḫ': 300, '→': 264, 'È': 206, 'ś': 303, '4': 261, 'π': 294, 'W': 305, 'é': 59, ':': 307, '‐': 309, 'ć': 270, 'Å': 311, '&': 312, 'k': 98, 'K': 314, '€': 315, 'м': 316, 'ě': 317, 'a': 318, 'ι': 319, 'ǒ': 320, 'O': 141, 'Π': 322, '李': 323, 'н': 283, 'ğ': 34, 'ن': 326, 'ß': 278, '2': 111}\n"
     ]
    }
   ],
   "source": [
    "char2Idx={}\n",
    "for char in characters:\n",
    "    char2Idx[char] = len(char2Idx)\n",
    "char2Idx['UNKNOWN'] = len(char2Idx)\n",
    "print(char2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Schartau', 'O'], ['sagte', 'O'], ['dem', 'O'], ['\"', 'O'], ['Tagesspiegel', 'O'], ['\"', 'O'], ['vom', 'O'], ['Freitag', 'O'], [',', 'O'], ['Fischer', 'O'], ['sei', 'O'], ['\"', 'O'], ['in', 'O'], ['einer', 'O'], ['Weise', 'O'], ['aufgetreten', 'O'], [',', 'O'], ['die', 'O'], ['alles', 'O'], ['andere', 'O'], ['als', 'O'], ['überzeugend', 'O'], ['war', 'O'], ['\"', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(trainSentences[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fastText.load_model(\"../embeddings/wiki.de.bin\")\n",
    "# ft = fastText.load_model(\"../embeddings/cc.de.300.bin\")\n",
    "\n",
    "nb_embedding_dims = ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "print(nb_embedding_dims)\n",
    "print(len(trainSentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Schartau', 'O'], ['sagte', 'O'], ['dem', 'O'], ['\"', 'O'], ['Tagesspiegel', 'O'], ['\"', 'O'], ['vom', 'O'], ['Freitag', 'O'], [',', 'O'], ['Fischer', 'O'], ['sei', 'O'], ['\"', 'O'], ['in', 'O'], ['einer', 'O'], ['Weise', 'O'], ['aufgetreten', 'O'], [',', 'O'], ['die', 'O'], ['alles', 'O'], ['andere', 'O'], ['als', 'O'], ['überzeugend', 'O'], ['war', 'O'], ['\"', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(trainSentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBatches(dataset):\n",
    "    l = []\n",
    "    for i in dataset:\n",
    "        l.append(len(i))\n",
    "    l = set(l)\n",
    "    print(len(l))\n",
    "    batches = []\n",
    "    batch_len = []\n",
    "    z = 0\n",
    "    for i in l:\n",
    "        temp = []\n",
    "        for batch in dataset:\n",
    "            if len(batch) == i:\n",
    "                temp.append(batch)\n",
    "                z += 1\n",
    "        batches.append(temp)\n",
    "#         batch_len.append(z)\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "45\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "train_batches = createBatches(trainSentences)\n",
    "dev_batches = createBatches(devSentences)\n",
    "test_batches = createBatches(testSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "3\n",
      "[[['Alles', 'O', 'O'], ['richtig', 'O', 'O'], ['.', 'O', 'O']], [['Farben', 'O', 'O'], ['eingeführt', 'O', 'O'], ['.', 'O', 'O']], [['Material', 'O', 'O'], ['gewinnen', 'O', 'O'], ['.', 'O', 'O']]]\n"
     ]
    }
   ],
   "source": [
    "batches = train_batches\n",
    "print(len(batches))\n",
    "print(len(batches[0]))\n",
    "print(batches[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(getCasing(\".\", case2Idx))\n",
    "print(L1embeddings[1])\n",
    "for input_data, output_data in generator(train_batches):\n",
    "    print(input_data[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(batches: 'list of training/dev sentences- batches already created'):\n",
    "    global line_number\n",
    "    \n",
    "    while True:\n",
    "        for batch in batches:\n",
    "            word_embeddings = []\n",
    "            case_embeddings = []\n",
    "            char_embeddings = []\n",
    "            nerlevel1_embeddings = []\n",
    "\n",
    "            output_labels = []\n",
    "            for index in range(len(batch)): # batches made according to the size of the sentences. len(batch) gives the size of current batch\n",
    "                sentence = batch[index]\n",
    "    #             print(sentence)\n",
    "                temp_casing = []\n",
    "                temp_char=[]\n",
    "                temp_word=[]\n",
    "                temp_output=[]\n",
    "                temp_nerlevel1_embeddings=[]\n",
    "                for word in sentence:\n",
    "                    word, label, L1 = word\n",
    "                    casing = getCasing(word, case2Idx)\n",
    "                    temp_casing.append(casing)\n",
    "                    temp_char2=[]\n",
    "                    for char in word:\n",
    "                        if char in char2Idx.keys():\n",
    "                            temp_char2.append(char2Idx[char])\n",
    "                        else:\n",
    "                            temp_char2.append(char2Idx['UNKNOWN']) # To incorporate the words which are not in the vocab\n",
    "                    temp_char2 = np.array(temp_char2)\n",
    "                    temp_char.append(temp_char2)\n",
    "                    word_vector = ft.get_word_vector(word.lower())\n",
    "                    # word_vector = ft.get_word_vector(word)\n",
    "                    temp_word.append(word_vector)\n",
    "                    temp_output.append(label2Idx[label])\n",
    "                    temp_nerlevel1_embeddings.append(labelL12Idx[L1])\n",
    "                temp_char = pad_sequences(temp_char, 52)\n",
    "                word_embeddings.append(temp_word)\n",
    "                case_embeddings.append(temp_casing)\n",
    "                char_embeddings.append(temp_char)\n",
    "                nerlevel1_embeddings.append(temp_nerlevel1_embeddings)\n",
    "                \n",
    "                temp_output = to_categorical(temp_output, len(label2Idx))\n",
    "                output_labels.append(temp_output)\n",
    "    #             output_labels = to_categorical()\n",
    "    #             output_labels = np.array(output_labels)\n",
    "    #             output_labels = output_labels[...,newaxis]\n",
    "\n",
    "    #             print(np.array(word_embeddings).shape)\n",
    "    #             print(np.array(case_embeddings).shape)\n",
    "    #             print(np.array(char_embeddings).shape)\n",
    "    #             print(output_labels.shape)\n",
    "    #             print(\"******************\\n\\n\")\n",
    "            yield ([np.array(word_embeddings), np.array(nerlevel1_embeddings), np.array(case_embeddings), np.array(char_embeddings)], np.array(output_labels))\n",
    "\n",
    "def get_label_from_categorical(a):\n",
    "    labels = []\n",
    "    for label in a:\n",
    "        label = np.ndarray.tolist(label)\n",
    "        label = np.argmax(label)\n",
    "        labels.append(label)\n",
    "    return(labels)\n",
    "\n",
    "def predict_batches(batch):\n",
    "    steps = 0\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for input_data, output_data in generator(batch):\n",
    "        pred_labels_batch = model.predict(input_data)\n",
    "        for s in pred_labels_batch:\n",
    "            pred_labels.append(get_label_from_categorical(s))\n",
    "        for s in output_data:\n",
    "            true_labels.append(get_label_from_categorical(s))\n",
    "        steps += 1\n",
    "        if steps == len(batch):\n",
    "            break\n",
    "    return(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2Label = {v: k for k, v in label2Idx.items()}\n",
    "idx2LabelL1 = {v: k for k, v in labelL12Idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for s in y_pred:\n",
    "        pred_labels.append(get_label_from_categorical(s))\n",
    "    for s in y_true:\n",
    "        true_labels.append(get_label_from_categorical(s))\n",
    "    p, r, f = compute_f1(y_pred, y_true, idx2Label)\n",
    "    return r\n",
    "\n",
    "def get_model():\n",
    "    words_input = Input(shape=(None, nb_embedding_dims), dtype='float32', name='words_input')\n",
    "    L1_input = Input(shape=(None,), dtype='int32', name='L1_input')\n",
    "    L1_embed = Embedding(output_dim=L1embeddings.shape[1], input_dim=L1embeddings.shape[0], weights=[L1embeddings], trainable=False, name = 'L1_embed')(L1_input)\n",
    "    casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
    "    casing = Embedding(output_dim=caseEmbeddings.shape[1], input_dim=caseEmbeddings.shape[0], weights=[caseEmbeddings], trainable=False, name = 'case_embed')(casing_input)\n",
    "    character_input=Input(shape=(None,52,),name='char_input')\n",
    "    embed_char_out=TimeDistributed(Embedding(len(char2Idx),32,embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input)\n",
    "    kernel_sizes = (3, 4, 5)\n",
    "    conv_blocks = []\n",
    "    for sz in kernel_sizes:\n",
    "        conv = TimeDistributed(Conv1D(\n",
    "                             kernel_size=sz,\n",
    "                             filters=32,\n",
    "                             padding=\"same\",\n",
    "                             activation=\"relu\",\n",
    "                             strides=1))(embed_char_out)\n",
    "        conv = TimeDistributed(MaxPooling1D(52))(conv)\n",
    "        conv = TimeDistributed(Flatten())(conv)\n",
    "        conv_blocks.append(conv)\n",
    "    output = concatenate([words_input, L1_embed, casing, conv_blocks[0], conv_blocks[1], conv_blocks[2]])\n",
    "    output = Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.5))(output)\n",
    "    output = TimeDistributed(Dense(len(label2Idx)))(output)\n",
    "    crf = CRF(len(label2Idx))\n",
    "    output = crf(output)\n",
    "    model = Model(inputs=[words_input, L1_input, casing_input, character_input], outputs=[output])\n",
    "    model.compile(loss=crf.loss_function, optimizer='nadam', metrics=[crf.accuracy])\n",
    "    model.summary()\n",
    "    return(model)\n",
    "\n",
    "class F1History(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.acc = []\n",
    "        self.f1_scores = []\n",
    "        self.max_f1 = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.acc.append(logs.get('val_acc'))\n",
    "        true_labels, pred_labels = predict_batches(dev_batches)\n",
    "        pre, rec, f1 = compute_f1(pred_labels, true_labels, idx2Label)\n",
    "        self.f1_scores.append(f1)\n",
    "        if epoch > 30 and f1 > self.max_f1:\n",
    "            print(\"\\nNew maximum F1 score: \" + str(f1) + \" (before: \" + str(self.max_f1) + \") Saving to \" + tmp_model_filename)\n",
    "            self.max_f1 = f1\n",
    "            model.save(tmp_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, None, 52)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, None, 52, 32) 10528       char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_90 (TimeDistri (None, None, 52, 32) 3104        char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_93 (TimeDistri (None, None, 52, 32) 4128        char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_96 (TimeDistri (None, None, 52, 32) 5152        char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "L1_input (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "casing_input (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_91 (TimeDistri (None, None, 1, 32)  0           time_distributed_90[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_94 (TimeDistri (None, None, 1, 32)  0           time_distributed_93[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_97 (TimeDistri (None, None, 1, 32)  0           time_distributed_96[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "words_input (InputLayer)        (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "L1_embed (Embedding)            (None, None, 25)     625         L1_input[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "case_embed (Embedding)          (None, None, 8)      64          casing_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_92 (TimeDistri (None, None, 32)     0           time_distributed_91[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_95 (TimeDistri (None, None, 32)     0           time_distributed_94[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_98 (TimeDistri (None, None, 32)     0           time_distributed_97[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, None, 429)    0           words_input[0][0]                \n",
      "                                                                 L1_embed[0][0]                   \n",
      "                                                                 case_embed[0][0]                 \n",
      "                                                                 time_distributed_92[0][0]        \n",
      "                                                                 time_distributed_95[0][0]        \n",
      "                                                                 time_distributed_98[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, None, 400)    1008000     concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_99 (TimeDistri (None, None, 18)     7218        bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "crf_9 (CRF)                     (None, None, 18)     702         time_distributed_99[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 1,039,521\n",
      "Trainable params: 1,038,832\n",
      "Non-trainable params: 689\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/80\n",
      "51/51 [==============================] - 28s 556ms/step - loss: 0.1464 - acc: 0.9744 - val_loss: 0.0332 - val_acc: 0.9947\n",
      "Epoch 2/80\n",
      "51/51 [==============================] - 22s 431ms/step - loss: 0.0495 - acc: 0.9916 - val_loss: 0.0294 - val_acc: 0.9947\n",
      "Epoch 3/80\n",
      "51/51 [==============================] - 23s 446ms/step - loss: 0.0382 - acc: 0.9918 - val_loss: 0.0313 - val_acc: 0.9955\n",
      "Epoch 4/80\n",
      "51/51 [==============================] - 22s 435ms/step - loss: 0.0272 - acc: 0.9926 - val_loss: 0.0308 - val_acc: 0.9938\n",
      "Epoch 5/80\n",
      "51/51 [==============================] - 22s 431ms/step - loss: 0.0194 - acc: 0.9937 - val_loss: 0.0055 - val_acc: 0.9958\n",
      "Epoch 6/80\n",
      "51/51 [==============================] - 22s 433ms/step - loss: 0.0113 - acc: 0.9942 - val_loss: 0.0714 - val_acc: 0.9777\n",
      "Epoch 7/80\n",
      "51/51 [==============================] - 22s 434ms/step - loss: 0.0049 - acc: 0.9949 - val_loss: 0.0212 - val_acc: 0.9879\n",
      "Epoch 8/80\n",
      "51/51 [==============================] - 22s 426ms/step - loss: -7.5192e-04 - acc: 0.9950 - val_loss: 0.0134 - val_acc: 0.9902\n",
      "Epoch 9/80\n",
      "51/51 [==============================] - 22s 430ms/step - loss: -0.0065 - acc: 0.9958 - val_loss: -0.0127 - val_acc: 0.9961\n",
      "Epoch 10/80\n",
      "51/51 [==============================] - 22s 436ms/step - loss: -0.0127 - acc: 0.9964 - val_loss: -0.0168 - val_acc: 0.9964\n",
      "Epoch 11/80\n",
      "51/51 [==============================] - 22s 435ms/step - loss: -0.0162 - acc: 0.9963 - val_loss: -0.0224 - val_acc: 0.9964\n",
      "Epoch 12/80\n",
      "51/51 [==============================] - 22s 441ms/step - loss: -0.0217 - acc: 0.9966 - val_loss: -0.0274 - val_acc: 0.9966\n",
      "Epoch 13/80\n",
      "51/51 [==============================] - 23s 453ms/step - loss: -0.0275 - acc: 0.9971 - val_loss: -0.0322 - val_acc: 0.9968\n",
      "Epoch 14/80\n",
      "51/51 [==============================] - 22s 432ms/step - loss: -0.0319 - acc: 0.9970 - val_loss: -0.0313 - val_acc: 0.9959\n",
      "Epoch 15/80\n",
      "51/51 [==============================] - 22s 425ms/step - loss: -0.0364 - acc: 0.9972 - val_loss: -0.0421 - val_acc: 0.9971\n",
      "Epoch 16/80\n",
      "51/51 [==============================] - 22s 438ms/step - loss: -0.0412 - acc: 0.9974 - val_loss: -0.0410 - val_acc: 0.9963\n",
      "Epoch 17/80\n",
      "51/51 [==============================] - 22s 430ms/step - loss: -0.0450 - acc: 0.9972 - val_loss: -0.0493 - val_acc: 0.9970\n",
      "Epoch 18/80\n",
      "51/51 [==============================] - 22s 431ms/step - loss: -0.0498 - acc: 0.9974 - val_loss: -0.0547 - val_acc: 0.9969\n",
      "Epoch 19/80\n",
      "51/51 [==============================] - 22s 436ms/step - loss: -0.0541 - acc: 0.9975 - val_loss: -0.0600 - val_acc: 0.9972\n",
      "Epoch 20/80\n",
      "51/51 [==============================] - 22s 432ms/step - loss: -0.0588 - acc: 0.9977 - val_loss: -0.0561 - val_acc: 0.9944\n",
      "Epoch 21/80\n",
      "51/51 [==============================] - 23s 445ms/step - loss: -0.0619 - acc: 0.9975 - val_loss: -0.0634 - val_acc: 0.9965\n",
      "Epoch 22/80\n",
      "51/51 [==============================] - 22s 440ms/step - loss: -0.0655 - acc: 0.9972 - val_loss: -0.0679 - val_acc: 0.9954\n",
      "Epoch 23/80\n",
      "51/51 [==============================] - 22s 429ms/step - loss: -0.0710 - acc: 0.9976 - val_loss: -0.0739 - val_acc: 0.9968\n",
      "Epoch 24/80\n",
      "51/51 [==============================] - 22s 431ms/step - loss: -0.0758 - acc: 0.9977 - val_loss: -0.0799 - val_acc: 0.9969\n",
      "Epoch 25/80\n",
      "51/51 [==============================] - 22s 438ms/step - loss: -0.0808 - acc: 0.9980 - val_loss: -0.0861 - val_acc: 0.9970\n",
      "Epoch 26/80\n",
      "51/51 [==============================] - 22s 431ms/step - loss: -0.0850 - acc: 0.9979 - val_loss: -0.0891 - val_acc: 0.9968\n",
      "Epoch 27/80\n",
      "51/51 [==============================] - 22s 439ms/step - loss: -0.0894 - acc: 0.9980 - val_loss: -0.0945 - val_acc: 0.9969\n",
      "Epoch 28/80\n",
      "51/51 [==============================] - 23s 449ms/step - loss: -0.0940 - acc: 0.9981 - val_loss: -0.1000 - val_acc: 0.9970\n",
      "Epoch 29/80\n",
      "51/51 [==============================] - 22s 431ms/step - loss: -0.0984 - acc: 0.9982 - val_loss: -0.1044 - val_acc: 0.9971\n",
      "Epoch 30/80\n",
      "51/51 [==============================] - 22s 431ms/step - loss: -0.1025 - acc: 0.9982 - val_loss: -0.1073 - val_acc: 0.9963\n",
      "Epoch 31/80\n",
      "51/51 [==============================] - 22s 436ms/step - loss: -0.1065 - acc: 0.9982 - val_loss: -0.1128 - val_acc: 0.9970\n",
      "Epoch 32/80\n",
      "50/51 [============================>.] - ETA: 0s - loss: -0.1127 - acc: 0.9983\n",
      "New maximum F1 score: 0.6631016042780749 (before: 0) Saving to tmp_generator_NER_best.h5\n",
      "51/51 [==============================] - 25s 481ms/step - loss: -0.1113 - acc: 0.9983 - val_loss: -0.1174 - val_acc: 0.9971\n",
      "Epoch 33/80\n",
      "50/51 [============================>.] - ETA: 0s - loss: -0.1174 - acc: 0.9984\n",
      "New maximum F1 score: 0.6777251184834123 (before: 0.6631016042780749) Saving to tmp_generator_NER_best.h5\n",
      "51/51 [==============================] - 22s 439ms/step - loss: -0.1160 - acc: 0.9984 - val_loss: -0.1227 - val_acc: 0.9971\n",
      "Epoch 34/80\n",
      "51/51 [==============================] - 22s 433ms/step - loss: -0.1204 - acc: 0.9985 - val_loss: -0.1232 - val_acc: 0.9966\n",
      "Epoch 35/80\n",
      "51/51 [==============================] - 22s 439ms/step - loss: -0.1247 - acc: 0.9985 - val_loss: -0.1309 - val_acc: 0.9971\n",
      "Epoch 36/80\n",
      "50/51 [============================>.] - ETA: 0s - loss: -0.1307 - acc: 0.9985\n",
      "New maximum F1 score: 0.6791443850267381 (before: 0.6777251184834123) Saving to tmp_generator_NER_best.h5\n",
      "51/51 [==============================] - 23s 447ms/step - loss: -0.1291 - acc: 0.9985 - val_loss: -0.1357 - val_acc: 0.9972\n",
      "Epoch 37/80\n",
      "51/51 [==============================] - 22s 436ms/step - loss: -0.1335 - acc: 0.9985 - val_loss: -0.1405 - val_acc: 0.9972\n",
      "Epoch 38/80\n",
      "51/51 [==============================] - 23s 443ms/step - loss: -0.1379 - acc: 0.9987 - val_loss: -0.1443 - val_acc: 0.9971\n",
      "Epoch 39/80\n",
      "51/51 [==============================] - 22s 436ms/step - loss: -0.1421 - acc: 0.9987 - val_loss: -0.1486 - val_acc: 0.9971\n",
      "Epoch 40/80\n",
      "50/51 [============================>.] - ETA: 0s - loss: -0.1484 - acc: 0.9987\n",
      "New maximum F1 score: 0.6810810810810811 (before: 0.6791443850267381) Saving to tmp_generator_NER_best.h5\n",
      "51/51 [==============================] - 23s 444ms/step - loss: -0.1466 - acc: 0.9987 - val_loss: -0.1530 - val_acc: 0.9973\n",
      "Epoch 41/80\n",
      "51/51 [==============================] - 22s 439ms/step - loss: -0.1507 - acc: 0.9986 - val_loss: -0.1572 - val_acc: 0.9970\n",
      "Epoch 42/80\n",
      "50/51 [============================>.] - ETA: 0s - loss: -0.1570 - acc: 0.9987\n",
      "New maximum F1 score: 0.6878048780487804 (before: 0.6810810810810811) Saving to tmp_generator_NER_best.h5\n",
      "51/51 [==============================] - 22s 440ms/step - loss: -0.1551 - acc: 0.9987 - val_loss: -0.1633 - val_acc: 0.9972\n",
      "Epoch 43/80\n",
      "51/51 [==============================] - 22s 438ms/step - loss: -0.1596 - acc: 0.9988 - val_loss: -0.1671 - val_acc: 0.9972\n",
      "Epoch 44/80\n",
      "51/51 [==============================] - 22s 428ms/step - loss: -0.1640 - acc: 0.9988 - val_loss: -0.1718 - val_acc: 0.9971\n",
      "Epoch 45/80\n",
      "50/51 [============================>.] - ETA: 0s - loss: -0.1705 - acc: 0.9989\n",
      "New maximum F1 score: 0.694789081885856 (before: 0.6878048780487804) Saving to tmp_generator_NER_best.h5\n",
      "51/51 [==============================] - 22s 438ms/step - loss: -0.1684 - acc: 0.9989 - val_loss: -0.1772 - val_acc: 0.9972\n",
      "Epoch 46/80\n",
      "51/51 [==============================] - 22s 436ms/step - loss: -0.1728 - acc: 0.9989 - val_loss: -0.1809 - val_acc: 0.9970\n",
      "Epoch 47/80\n",
      "51/51 [==============================] - 22s 435ms/step - loss: -0.1769 - acc: 0.9988 - val_loss: -0.1859 - val_acc: 0.9971\n",
      "Epoch 48/80\n",
      "51/51 [==============================] - 22s 429ms/step - loss: -0.1815 - acc: 0.9990 - val_loss: -0.1904 - val_acc: 0.9971\n",
      "Epoch 49/80\n",
      "51/51 [==============================] - 22s 435ms/step - loss: -0.1858 - acc: 0.9990 - val_loss: -0.1946 - val_acc: 0.9971\n",
      "Epoch 50/80\n",
      "51/51 [==============================] - 22s 438ms/step - loss: -0.1902 - acc: 0.9990 - val_loss: -0.1994 - val_acc: 0.9971\n",
      "Epoch 51/80\n",
      "50/51 [============================>.] - ETA: 0s - loss: -0.1970 - acc: 0.9991\n",
      "New maximum F1 score: 0.6982543640897756 (before: 0.694789081885856) Saving to tmp_generator_NER_best.h5\n",
      "51/51 [==============================] - 22s 435ms/step - loss: -0.1946 - acc: 0.9991 - val_loss: -0.2037 - val_acc: 0.9972\n",
      "Epoch 52/80\n",
      "51/51 [==============================] - 22s 435ms/step - loss: -0.1990 - acc: 0.9991 - val_loss: -0.2078 - val_acc: 0.9969\n",
      "Epoch 53/80\n",
      "51/51 [==============================] - 22s 435ms/step - loss: -0.2031 - acc: 0.9990 - val_loss: -0.2105 - val_acc: 0.9971\n",
      "Epoch 54/80\n",
      "51/51 [==============================] - 22s 429ms/step - loss: -0.2073 - acc: 0.9990 - val_loss: -0.2164 - val_acc: 0.9969\n",
      "Epoch 55/80\n",
      "51/51 [==============================] - 22s 437ms/step - loss: -0.2117 - acc: 0.9991 - val_loss: -0.2212 - val_acc: 0.9970\n",
      "Epoch 56/80\n",
      "50/51 [============================>.] - ETA: 0s - loss: -0.2188 - acc: 0.9991\n",
      "New maximum F1 score: 0.7024390243902439 (before: 0.6982543640897756) Saving to tmp_generator_NER_best.h5\n",
      "51/51 [==============================] - 22s 437ms/step - loss: -0.2161 - acc: 0.9991 - val_loss: -0.2261 - val_acc: 0.9973\n",
      "Epoch 57/80\n",
      "51/51 [==============================] - 22s 440ms/step - loss: -0.2207 - acc: 0.9993 - val_loss: -0.2306 - val_acc: 0.9971\n",
      "Epoch 58/80\n",
      "51/51 [==============================] - 22s 431ms/step - loss: -0.2249 - acc: 0.9992 - val_loss: -0.2348 - val_acc: 0.9972\n",
      "Epoch 59/80\n",
      "50/51 [============================>.] - ETA: 0s - loss: -0.2322 - acc: 0.9993\n",
      "New maximum F1 score: 0.7072599531615924 (before: 0.7024390243902439) Saving to tmp_generator_NER_best.h5\n",
      "51/51 [==============================] - 22s 439ms/step - loss: -0.2293 - acc: 0.9993 - val_loss: -0.2400 - val_acc: 0.9973\n",
      "Epoch 60/80\n",
      "51/51 [==============================] - 22s 436ms/step - loss: -0.2336 - acc: 0.9993 - val_loss: -0.2438 - val_acc: 0.9972\n",
      "Epoch 61/80\n",
      "51/51 [==============================] - 22s 429ms/step - loss: -0.2379 - acc: 0.9993 - val_loss: -0.2475 - val_acc: 0.9972\n",
      "Epoch 62/80\n",
      "51/51 [==============================] - 22s 432ms/step - loss: -0.2421 - acc: 0.9993 - val_loss: -0.2518 - val_acc: 0.9970\n",
      "Epoch 63/80\n",
      "51/51 [==============================] - 22s 440ms/step - loss: -0.2457 - acc: 0.9991 - val_loss: -0.2559 - val_acc: 0.9966\n",
      "Epoch 64/80\n",
      "51/51 [==============================] - 22s 431ms/step - loss: -0.2502 - acc: 0.9991 - val_loss: -0.2623 - val_acc: 0.9972\n",
      "Epoch 65/80\n",
      "51/51 [==============================] - 23s 444ms/step - loss: -0.2550 - acc: 0.9993 - val_loss: -0.2648 - val_acc: 0.9967\n",
      "Epoch 66/80\n",
      "51/51 [==============================] - 22s 437ms/step - loss: -0.2594 - acc: 0.9994 - val_loss: -0.2707 - val_acc: 0.9973\n",
      "Epoch 67/80\n",
      "50/51 [============================>.] - ETA: 0s - loss: -0.2670 - acc: 0.9994\n",
      "New maximum F1 score: 0.7135922330097088 (before: 0.7072599531615924) Saving to tmp_generator_NER_best.h5\n",
      "51/51 [==============================] - 22s 439ms/step - loss: -0.2637 - acc: 0.9994 - val_loss: -0.2755 - val_acc: 0.9974\n",
      "Epoch 68/80\n",
      "51/51 [==============================] - 22s 436ms/step - loss: -0.2680 - acc: 0.9994 - val_loss: -0.2797 - val_acc: 0.9973\n",
      "Epoch 69/80\n",
      "51/51 [==============================] - 22s 438ms/step - loss: -0.2723 - acc: 0.9995 - val_loss: -0.2846 - val_acc: 0.9973\n",
      "Epoch 70/80\n",
      "51/51 [==============================] - 22s 438ms/step - loss: -0.2767 - acc: 0.9995 - val_loss: -0.2882 - val_acc: 0.9972\n",
      "Epoch 71/80\n",
      "51/51 [==============================] - 22s 433ms/step - loss: -0.2810 - acc: 0.9995 - val_loss: -0.2927 - val_acc: 0.9973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/80\n",
      "51/51 [==============================] - 22s 429ms/step - loss: -0.2852 - acc: 0.9995 - val_loss: -0.2966 - val_acc: 0.9969\n",
      "Epoch 73/80\n",
      "51/51 [==============================] - 23s 441ms/step - loss: -0.2893 - acc: 0.9995 - val_loss: -0.3011 - val_acc: 0.9974\n",
      "Epoch 74/80\n",
      "51/51 [==============================] - 22s 436ms/step - loss: -0.2936 - acc: 0.9994 - val_loss: -0.3063 - val_acc: 0.9971\n",
      "Epoch 75/80\n",
      "51/51 [==============================] - 23s 450ms/step - loss: -0.2980 - acc: 0.9995 - val_loss: -0.3100 - val_acc: 0.9971\n",
      "Epoch 76/80\n",
      "51/51 [==============================] - 23s 446ms/step - loss: -0.3020 - acc: 0.9993 - val_loss: -0.3112 - val_acc: 0.9971\n",
      "Epoch 77/80\n",
      "51/51 [==============================] - 22s 435ms/step - loss: -0.3049 - acc: 0.9991 - val_loss: -0.3189 - val_acc: 0.9972\n",
      "Epoch 78/80\n",
      "50/51 [============================>.] - ETA: 0s - loss: -0.3143 - acc: 0.9994\n",
      "New maximum F1 score: 0.7233009708737864 (before: 0.7135922330097088) Saving to tmp_generator_NER_best.h5\n",
      "51/51 [==============================] - 22s 439ms/step - loss: -0.3104 - acc: 0.9994 - val_loss: -0.3254 - val_acc: 0.9974\n",
      "Epoch 79/80\n",
      "51/51 [==============================] - 22s 437ms/step - loss: -0.3151 - acc: 0.9995 - val_loss: -0.3296 - val_acc: 0.9973\n",
      "Epoch 80/80\n",
      "51/51 [==============================] - 22s 439ms/step - loss: -0.3191 - acc: 0.9995 - val_loss: -0.3278 - val_acc: 0.9965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f755c7cbcc0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_model_filename = 'tmp_generator_NER_best.h5'\n",
    "# checkpoint = ModelCheckpoint(tmp_model_filename, verbose=1, save_best_only = True, monitor = 'val_acc')\n",
    "history = F1History()\n",
    "model = get_model()\n",
    "model.fit_generator(\n",
    "    generator(train_batches), \n",
    "    epochs = 80, steps_per_epoch = len(train_batches), \n",
    "    validation_data = generator(dev_batches), validation_steps = len(dev_batches), \n",
    "    callbacks = [history]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9946695905111053, 0.9946695905111053, 0.9753653725981712, 0.971133625805378, 0.9892300726066936, 0.9916527701778846, 0.9934801145033403, 0.9193957837332378, 0.9945173840360207, 0.9694857255708088, 0.9949436785416169, 0.9296631378206339, 0.9950203010981733, 0.9947294317050414, 0.9950850945711136, 0.9952510368282145, 0.9952144384113225, 0.9952569980783896, 0.9953162610530853, 0.9944079532135617, 0.995281142429872, 0.9953866038810123, 0.9953606687350707, 0.9954716947945681, 0.9955632332509214, 0.995618404041637, 0.9956095008416609, 0.9956665044752034, 0.9953894490545446, 0.9953945659778335, 0.995706004283645, 0.9957643909887834, 0.9956305682930079, 0.9957253030213443, 0.9958990923112089, 0.9957777957211841, 0.9955365413427353, 0.9957157587734136, 0.9956198622421785, 0.9956063252958385, 0.9958345579559152, 0.9956996938586236, 0.9958652172847228, 0.9959894129092043, 0.9959836213155226, 0.9959905016151341, 0.9959326248277317, 0.9959960159659386, 0.9962591779232025, 0.9959572835402055, 0.9960178007591854, 0.9960247241637924, 0.9961614792726257, 0.9959337345307524, 0.9960361340641976, 0.9958904425664381, 0.9960068343715234, 0.9958731552145698, 0.9956828972697258, 0.995921416580677, 0.9959934198856354, 0.9959670436653224, 0.995522012385455, 0.9114039875702424, 0.9948186420581557, 0.9102330112186345, 0.9950387359478257, 0.9955177603526549, 0.9961365892670372, 0.9961044318567622, 0.9962850817225196, 0.9960857303305106, 0.9961196438832717, 0.9958124020695687, 0.9952916812354868, 0.9956907460906289, 0.9962496629628268, 0.9956224945458498, 0.9955316628922116, 0.9961185671524568]\n",
      "[0, 0, 0.1321455085374907, 0.11959287531806617, 0.2818181818181818, 0.30468749999999994, 0.33004926108374383, 0.06942889137737962, 0.2626262626262626, 0.15420200462606012, 0.10666666666666666, 0.052261306532663324, 0.208, 0.43271767810026385, 0.16239316239316237, 0.4194528875379939, 0.42633228840125387, 0.3223443223443223, 0.3475177304964539, 0.41791044776119407, 0.23387096774193547, 0.26666666666666666, 0.33935018050541516, 0.3525179856115108, 0.3206106870229008, 0.3542435424354244, 0.35766423357664234, 0.42857142857142855, 0.5169712793733682, 0.3394833948339484, 0.42384105960264895, 0.42483660130718953, 0.39024390243902435, 0.4662576687116564, 0.5120481927710843, 0.4899135446685879, 0.41558441558441556, 0.515759312320917, 0.45625000000000004, 0.3571428571428571, 0.47352024922118385, 0.5212464589235127, 0.4804804804804806, 0.5480225988700564, 0.5389221556886227, 0.49358974358974356, 0.5685279187817259, 0.5474860335195532, 0.5872576177285318, 0.5833333333333333, 0.5540166204986149, 0.5699208443271768, 0.5698324022346369, 0.5759162303664921, 0.5344352617079889, 0.5670886075949367, 0.5591397849462365, 0.5513513513513514, 0.5497630331753556, 0.588235294117647, 0.5728900255754475, 0.5750636132315521, 0.5268542199488491, 0.13911060433295325, 0.07272727272727272, 0.04491091042226019, 0.12875536480686695, 0.5576923076923078, 0.5929648241206029, 0.5873417721518989, 0.5633802816901409, 0.587378640776699, 0.5604395604395604, 0.5849056603773585, 0.5726681127982647, 0.580335731414868, 0.5922077922077922, 0.5700712589073633, 0.5740740740740741, 0.582089552238806]\n"
     ]
    }
   ],
   "source": [
    "print(history.acc)\n",
    "print(history.f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(tmp_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels, pred_labels = predict_batches(test_batches)\n",
    "print(compute_f1(pred_labels, true_labels, idx2Label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('results_wiki.txt', 'w')\n",
    "for run_i in range(13):\n",
    "    print(\"Run \" + str(run_i))\n",
    "    \n",
    "    tmp_model_filename = 'tmp_generator_NER_best.' + str(run_i) + '.h5'\n",
    "    # tmp_model_filename = 'tmp_generator_NER_best.h5'\n",
    "    # checkpoint = ModelCheckpoint(tmp_model_filename, verbose=1, save_best_only = True, monitor = 'val_acc')\n",
    "    history = F1History()\n",
    "    \n",
    "    model = get_model()\n",
    "    model.fit_generator(\n",
    "        generator(train_batches), \n",
    "        epochs = 80, steps_per_epoch = len(train_batches), \n",
    "        validation_data = generator(dev_batches), validation_steps = len(dev_batches), \n",
    "        callbacks = [history]\n",
    "    )\n",
    "    \n",
    "    model.load_weights(tmp_model_filename)\n",
    "    true_labels, pred_labels = predict_batches(test_batches)\n",
    "    \n",
    "    pre, rec, f1 = compute_f1(pred_labels, true_labels, idx2Label)\n",
    "    f.write(str(run_i) + \"\\t\" + str(pre) + \"\\t\" + str(rec) +  \"\\t\" + str(f1))\n",
    "    f.write(\"\\n\")\n",
    "    f.flush()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, json\n",
    "# copy file for best run\n",
    "shutil.copyfile('tmp_generator_NER_best.0.h5', 'final_model_germeval.h5')\n",
    "with open(\"final_model_germeval.indexes\", \"w\") as f:\n",
    "    json.dump([idx2Label, label2Idx, char2Idx, case2Idx], f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
