{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np \n",
    "from keras.utils import to_categorical\n",
    "from validation import compute_f1\n",
    "from keras.models import Model\n",
    "from keras.layers import TimeDistributed,Conv1D,Dense,Embedding,Input,Dropout,LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate\n",
    "# from prepro import readfile,createBatches,createMatrices,iterate_minibatches,addCharInformatioin,padding\n",
    "from keras.utils import plot_model,Progbar\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "from keras_contrib.layers import CRF\n",
    "from numpy import newaxis\n",
    "from random import shuffle\n",
    "import math\n",
    "import sklearn\n",
    "import subprocess\n",
    "import fastText\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCasing(word, caseLookup):\n",
    "    \n",
    "    if word == 'PADDING_TOKEN':\n",
    "        return(caseLookup['PADDING_TOKEN'])\n",
    "    \n",
    "    casing = 'other'\n",
    "    \n",
    "    numDigits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            numDigits += 1\n",
    "            \n",
    "    digitFraction = numDigits / float(len(word))\n",
    "    \n",
    "    if word.isdigit(): #Is a digit\n",
    "        casing = 'numeric'\n",
    "    elif digitFraction > 0.5:\n",
    "        casing = 'mainly_numeric'\n",
    "    elif word.islower(): #All lower case\n",
    "        casing = 'allLower'\n",
    "    elif word.isupper(): #All upper case\n",
    "        casing = 'allUpper'\n",
    "    elif word[0].isupper(): #is a title, initial char upper, then all lower\n",
    "        casing = 'initialUpper'\n",
    "    elif numDigits > 0:\n",
    "        casing = 'contains_digit'\n",
    "   \n",
    "    return caseLookup[casing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_dataset(dataset):\n",
    "    correctLabels = []\n",
    "    predLabels = []\n",
    "    b = Progbar(len(dataset))\n",
    "    for i,data in enumerate(dataset):    \n",
    "        tokens, casing,char, labels = data\n",
    "        tokens = np.asarray([tokens])     \n",
    "        casing = np.asarray([casing])\n",
    "        char = np.asarray([char])\n",
    "        pred = model.predict([tokens, casing,char], verbose=False)[0]   \n",
    "        pred = pred.argmax(axis=-1) #Predict the classes            \n",
    "        correctLabels.append(labels)\n",
    "        predLabels.append(pred)\n",
    "        b.update(i)\n",
    "    return predLabels, correctLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing all deriv and part to misc. with BIO\n",
    "def modify_labels(dataset):\n",
    "    bad_labels = ['I-PERderiv','I-OTHpart','B-ORGderiv', 'I-OTH','B-OTHpart','B-LOCderiv','I-LOCderiv','I-OTHderiv','B-PERderiv','B-OTHderiv','B-PERpart','I-PERpart','I-LOCpart','B-LOCpart','I-ORGpart','I-ORGderiv','B-ORGpart','B-OTH']\n",
    "    for sentence in dataset:\n",
    "        for word in sentence:\n",
    "            label = word[1]\n",
    "            if label in bad_labels:\n",
    "                first_char = label[0]\n",
    "                if first_char == 'B' :\n",
    "                    word[1] = 'B-MISC'\n",
    "                else:\n",
    "                    word[1] = 'I-MISC'\n",
    "    return dataset\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_germeval(path):\n",
    "    sentences = []\n",
    "    with open(path, 'r', encoding = 'UTF-8') as f:\n",
    "        sentence = []\n",
    "        for line in f:\n",
    "            \n",
    "            line = line.strip()\n",
    "            \n",
    "            # append sentence\n",
    "            if len(line) == 0:\n",
    "                if len(sentence):\n",
    "                    sentences.append(sentence)\n",
    "                sentence = []\n",
    "                continue\n",
    "            \n",
    "            # get sentence tokens\n",
    "            splits = line.split()\n",
    "            if splits[0] == '#':\n",
    "                continue\n",
    "            temp = [splits[1],splits[2]]\n",
    "            sentence.append(temp)\n",
    "        \n",
    "        # append last\n",
    "        if len(sentence):\n",
    "            sentences.append(sentence)    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preproecessing data from Conll\n",
    "def get_sentences_conll(filename):\n",
    "    '''\n",
    "        -DOCSTART- -X- -X- O\n",
    "\n",
    "    EU NNP B-NP B-ORG\n",
    "    rejects VBZ B-VP O\n",
    "    German JJ B-NP B-MISC\n",
    "    call NN I-NP O\n",
    "    to TO B-VP O\n",
    "    boycott VB I-VP O\n",
    "    British JJ B-NP B-MISC\n",
    "    lamb NN I-NP O\n",
    "    . . O O\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    read file\n",
    "    return format :\n",
    "    [ ['EU', 'B-ORG'], ['rejects', 'O'], ['German', 'B-MISC'], ['call', 'O'], ['to', 'O'], ['boycott', 'O'], ['British', 'B-MISC'], ['lamb', 'O'], ['.', 'O'] ]\n",
    "    '''\n",
    "    f = open(filename,'rb')\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in f:\n",
    "        splits = line.split()\n",
    "        try:\n",
    "            word=splits[0].decode()\n",
    "            if word=='-DOCSTART-':\n",
    "                continue\n",
    "            label=splits[-1].decode()\n",
    "            temp=[word,label]\n",
    "            sentence.append(temp)\n",
    "        except Exception as e:\n",
    "            if len(sentence)!=0:\n",
    "                sentences.append(sentence)\n",
    "                sentence=[]\n",
    "    return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000\n",
      "2200\n",
      "5100\n"
     ]
    }
   ],
   "source": [
    "trainSentences = get_sentences_germeval('../data/GermEVAL/NER-de-train.tsv')\n",
    "devSentences = get_sentences_germeval('../data/GermEVAL/NER-de-dev.tsv')\n",
    "testSentences = get_sentences_germeval('../data/GermEVAL/NER-de-test.tsv')\n",
    "\n",
    "# trainSentences = get_sentences('../data/CONLL/deu/deu_utf.train')\n",
    "# devSentences = get_sentences('../data/CONLL/deu/deu_utf.testa')\n",
    "# testSentences = get_sentences('../data/CONLL/deu/deu_utf.testb')\n",
    "\n",
    "print(len(trainSentences))\n",
    "print(len(devSentences))\n",
    "print(len(testSentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1951', 'O'], ['bis', 'O'], ['1953', 'O'], ['wurde', 'O'], ['der', 'O'], ['nördliche', 'O'], ['Teil', 'O'], ['als', 'O'], ['Jugendburg', 'O'], ['des', 'O'], ['Kolpingwerkes', 'B-OTH'], ['gebaut', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(testSentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelSet = set()\n",
    "characters= set()\n",
    "max_sequence_length = 0\n",
    "\n",
    "for dataset in [trainSentences, devSentences, testSentences]:\n",
    "    for sentence in dataset:\n",
    "        for word, label in sentence:\n",
    "            for char in word:\n",
    "                characters.add(char)\n",
    "            labelSet.add(label)\n",
    "        if len(sentence) > max_sequence_length:\n",
    "            max_sequence_length = len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "print(len(labelSet))\n",
    "print(max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Create a mapping for the labels ::\n",
    "label2Idx = {\"PADDING_TOKEN\":0}\n",
    "for label in labelSet:\n",
    "    label2Idx[label] = len(label2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-OTHpart': 1, 'I-LOCderiv': 13, 'B-OTH': 3, 'B-PER': 14, 'I-PERderiv': 25, 'B-ORGderiv': 4, 'O': 5, 'B-LOCderiv': 15, 'B-LOC': 16, 'B-PERderiv': 17, 'I-ORGpart': 18, 'I-ORG': 2, 'I-PER': 19, 'B-ORG': 20, 'PADDING_TOKEN': 0, 'B-OTHderiv': 7, 'B-ORGpart': 22, 'I-OTHpart': 23, 'I-LOCpart': 24, 'I-ORGderiv': 8, 'I-PERpart': 21, 'I-OTH': 9, 'I-OTHderiv': 10, 'B-LOCpart': 6, 'I-LOC': 11, 'B-PERpart': 12}\n"
     ]
    }
   ],
   "source": [
    "print(label2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Hard coded case lookup ::\n",
    "case2Idx = {'PADDING_TOKEN':0, 'numeric': 1, 'allLower':2, 'allUpper':3, 'initialUpper':4, 'other':5, 'mainly_numeric':6, 'contains_digit': 7}\n",
    "caseEmbeddings = np.identity(len(case2Idx), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "{'numeric': 1, 'allUpper': 3, 'mainly_numeric': 6, 'allLower': 2, 'contains_digit': 7, 'PADDING_TOKEN': 0, 'initialUpper': 4, 'other': 5}\n"
     ]
    }
   ],
   "source": [
    "print(caseEmbeddings)\n",
    "print(case2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(trainSentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Schartau', 'B-PER'], ['sagte', 'O'], ['dem', 'O'], ['\"', 'O'], ['Tagesspiegel', 'B-ORG'], ['\"', 'O'], ['vom', 'O'], ['Freitag', 'O'], [',', 'O'], ['Fischer', 'B-PER'], ['sei', 'O'], ['\"', 'O'], ['in', 'O'], ['einer', 'O'], ['Weise', 'O'], ['aufgetreten', 'O'], [',', 'O'], ['die', 'O'], ['alles', 'O'], ['andere', 'O'], ['als', 'O'], ['überzeugend', 'O'], ['war', 'O'], ['\"', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(trainSentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'υ': 1, 'ε': 2, '²': 3, '¹': 4, 'L': 5, 'ğ': 6, '+': 52, 'Î': 7, 'Е': 110, 'ệ': 8, 'î': 9, '\\x92': 286, 'Π': 11, 'à': 12, 'ź': 13, 'á': 14, '傳': 217, 'O': 16, '▪': 17, '寝': 325, 'м': 18, '„': 231, '東': 312, 'G': 21, 'B': 22, 'д': 25, 'Ġ': 27, '³': 26, 'ế': 235, '\\x9a': 28, 'ي': 169, ']': 30, '公': 31, 'Ö': 32, 'Ü': 33, 'γ': 314, '太': 35, '별': 36, 'У': 284, '`': 37, 'Å': 38, '算': 40, 'ċ': 255, 'ý': 42, 'â': 174, 'Ş': 44, 'h': 45, 'С': 46, '\\xad': 47, \"'\": 48, 'ą': 24, '$': 50, 'c': 51, '殿': 201, '別': 53, 'ó': 54, 'ø': 282, 'κ': 200, 'Ł': 55, 'ß': 56, 'ě': 57, '¤': 58, 'b': 59, '›': 60, 'û': 61, 't': 62, 'ú': 63, 'p': 64, 'ρ': 65, '−': 67, '«': 118, 'б': 68, 'z': 69, 'є': 70, '†': 287, 'ъ': 72, 'w': 96, 'F': 73, '博': 75, '’': 76, 'П': 78, '南': 79, 'ŏ': 15, 'ē': 80, 'ά': 295, 'Â': 82, '冲': 83, 'λ': 181, 'ī': 84, 'с': 293, 'α': 85, '©': 242, '·': 10, '%': 89, 'и': 90, '루': 259, '7': 92, 'é': 97, 'ῦ': 98, 'a': 244, 'ǒ': 100, 'φ': 99, '_': 106, '#': 102, 'E': 103, '&': 104, '柯': 185, 'œ': 107, '£': 108, 'Š': 109, 'ô': 111, 'ă': 112, '士': 113, '2': 114, 'ā': 116, 'В': 119, 'æ': 121, '/': 248, 'T': 122, '±': 19, 'η': 123, 'Z': 124, 'ĩ': 125, 'ħ': 126, ')': 275, 'أ': 127, 's': 94, '\\x94': 129, '동': 131, 'ü': 130, '李': 226, 'ð': 135, 'ś': 133, 'ş': 134, '-': 190, 'ن': 136, '§': 137, 'т': 296, 'ö': 86, 'ο': 138, 'Ø': 139, 'i': 251, '→': 140, 'р': 128, 'Æ': 224, '章': 144, 'X': 142, 'UNKNOWN': 329, 'Ц': 205, 'ب': 146, 'ь': 252, '‹': 154, 'č': 149, 'È': 304, 'd': 150, 'ł': 152, 'я': 156, 'ä': 153, '½': 155, 'Ż': 160, 'ž': 157, 'V': 158, '台': 29, 'å': 257, 'f': 161, 'ő': 164, '´': 194, '°': 165, 'ψ': 170, 'M': 167, 'п': 168, '″': 306, 'P': 171, '?': 172, '!': 173, 'Á': 175, '×': 117, '대': 323, 'ا': 177, 'W': 178, 'J': 179, 'j': 180, '\\u200e': 182, 'ō': 183, '\\x80': 184, '~': 319, 'ç': 186, 'з': 188, 'Q': 189, 'ḫ': 187, 'о': 191, 'σ': 192, 'ı': 309, 'л': 193, '⋅': 195, '4': 196, 'Œ': 197, '‚': 132, 'ć': 198, '造': 207, 'A': 202, '9': 203, 'М': 34, 'й': 264, '“': 209, '\\x95': 299, 'а': 310, 'v': 211, 'ν': 212, 'ñ': 213, 'ņ': 208, '}': 216, 'ό': 199, '@': 220, '8': 221, 'ы': 223, 'ـ': 225, 'Č': 206, '妃': 311, '¸': 307, 'I': 81, 'l': 228, '5': 204, 'ʻ': 71, 'S': 232, '0': 233, 'o': 234, '鷹': 237, 'ë': 238, '…': 66, 'µ': 239, '鶴': 240, 'к': 241, 'Ž': 105, '守': 258, 'Λ': 230, 'ř': 246, 'Ш': 269, 'Ä': 276, 'x': 23, '⊃': 249, '>': 313, ',': 250, '”': 39, 'у': 141, '.': 253, 'y': 254, 'g': 256, '九': 143, '6': 87, 'в': 260, '1': 261, 'ラ': 262, 'ū': 263, 'Т': 176, 'ň': 218, 'e': 265, 'u': 266, '€': 267, 'C': 268, ';': 145, 'ť': 270, 'έ': 214, 'И': 271, 'β': 74, 'n': 273, 'í': 91, 'ã': 274, 'İ': 215, '—': 277, 'N': 272, 'š': 41, '학': 278, '*': 147, 'U': 280, '»': 148, 'Y': 88, 'm': 93, '3': 285, 'R': 283, '佐': 227, 'r': 95, 'è': 288, 'х': 289, '‘': 290, '\\x96': 151, '[': 291, 'PADDING_TOKEN': 0, '≘': 292, '\"': 219, 'г': 294, 'Л': 43, '=': 247, 'ю': 300, 'É': 298, 'н': 222, 'オ': 324, '<': 301, '術': 320, 'K': 302, 'ї': 303, 'õ': 297, 'τ': 305, 'H': 229, 'ض': 162, 'À': 279, 'ê': 243, 'ę': 308, 'ɨ': 159, ':': 101, 'Þ': 166, '‐': 245, 'ς': 120, '≤': 20, 'е': 77, 'ḳ': 163, '樓': 236, 'D': 115, '\\x99': 315, '(': 316, '貴': 317, 'k': 318, '懿': 210, 'ż': 321, 'ń': 322, 'π': 281, 'ι': 49, 'q': 326, 'ж': 327, '–': 328}\n"
     ]
    }
   ],
   "source": [
    "char2Idx={\"PADDING_TOKEN\":0}\n",
    "for char in characters:\n",
    "    char2Idx[char] = len(char2Idx)\n",
    "char2Idx['UNKNOWN'] = len(char2Idx)\n",
    "print(char2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Schartau', 'B-PER'], ['sagte', 'O'], ['dem', 'O'], ['\"', 'O'], ['Tagesspiegel', 'B-ORG'], ['\"', 'O'], ['vom', 'O'], ['Freitag', 'O'], [',', 'O'], ['Fischer', 'B-PER'], ['sei', 'O'], ['\"', 'O'], ['in', 'O'], ['einer', 'O'], ['Weise', 'O'], ['aufgetreten', 'O'], [',', 'O'], ['die', 'O'], ['alles', 'O'], ['andere', 'O'], ['als', 'O'], ['überzeugend', 'O'], ['war', 'O'], ['\"', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(trainSentences[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fastText.load_model(\"../embeddings/wiki.de.bin\")\n",
    "# ft = fastText.load_model(\"../embeddings/cc.de.300.bin\")\n",
    "\n",
    "nb_embedding_dims = ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "print(nb_embedding_dims)\n",
    "print(len(trainSentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Schartau', 'B-PER'], ['sagte', 'O'], ['dem', 'O'], ['\"', 'O'], ['Tagesspiegel', 'B-ORG'], ['\"', 'O'], ['vom', 'O'], ['Freitag', 'O'], [',', 'O'], ['Fischer', 'B-PER'], ['sei', 'O'], ['\"', 'O'], ['in', 'O'], ['einer', 'O'], ['Weise', 'O'], ['aufgetreten', 'O'], [',', 'O'], ['die', 'O'], ['alles', 'O'], ['andere', 'O'], ['als', 'O'], ['überzeugend', 'O'], ['war', 'O'], ['\"', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(trainSentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBatches(dataset, batch_size):\n",
    "    batches = []\n",
    "    temp = []\n",
    "    i = 0\n",
    "    for item in dataset:\n",
    "        temp.append(item)\n",
    "        i += 1\n",
    "        if i == batch_size:\n",
    "            batches.append(temp)\n",
    "            temp = []\n",
    "            i = 0\n",
    "    if len(temp) > 0:\n",
    "        batches.append(temp)\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# train_batches = createBatches(trainSentences, batch_size)\n",
    "# dev_batches = createBatches(devSentences, batch_size)\n",
    "# test_batches = createBatches(testSentences, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(data, shuffle_data = False):\n",
    "    \n",
    "    print(\"Creating batches ...\")\n",
    "    batches = createBatches(data, batch_size)\n",
    "    \n",
    "    if shuffle_data:\n",
    "        print(\"Shuffling ...\")\n",
    "        shuffle(batches)\n",
    "    \n",
    "    while True:\n",
    "        for batch in batches:\n",
    "            word_embeddings = []\n",
    "            case_embeddings = []\n",
    "            char_embeddings = []\n",
    "            \n",
    "            output_labels = []\n",
    "            \n",
    "            # batches made according to the size of the sentences. len(batch) gives the size of current batch\n",
    "            for index in range(len(batch)): \n",
    "                sentence = batch[index]\n",
    "    #             print(sentence)\n",
    "                \n",
    "                temp_word= []\n",
    "                temp_casing = []\n",
    "                temp_char= []\n",
    "                \n",
    "                temp_output=[]\n",
    "                \n",
    "                # padding\n",
    "                words_to_pad = max_sequence_length - len(sentence)\n",
    "                for i in range(words_to_pad):\n",
    "                    sentence.append(['PADDING_TOKEN', 'PADDING_TOKEN'])\n",
    "                \n",
    "                # create data input for words\n",
    "                for word in sentence:\n",
    "                    word, label = word\n",
    "                    temp_output.append(label2Idx[label])\n",
    "                    \n",
    "                    casing = getCasing(word, case2Idx)\n",
    "                    temp_casing.append(casing)\n",
    "                        \n",
    "                    if word == 'PADDING_TOKEN':\n",
    "                        temp_char2=np.array([char2Idx['PADDING_TOKEN']])\n",
    "                        temp_char.append(temp_char2)\n",
    "                        word_vector = [0] * nb_embedding_dims\n",
    "                        temp_word.append(word_vector)\n",
    "                    else:\n",
    "                        # char\n",
    "                        temp_char2=[]\n",
    "                        for char in word:\n",
    "                            if char in char2Idx.keys():\n",
    "                                temp_char2.append(char2Idx[char])\n",
    "                            else:\n",
    "                                temp_char2.append(char2Idx['UNKNOWN']) # To incorporate the words which are not in the vocab\n",
    "                        temp_char2 = np.array(temp_char2)\n",
    "                        temp_char.append(temp_char2)\n",
    "                        \n",
    "                        # word\n",
    "                        word_vector = ft.get_word_vector(word.lower())\n",
    "                        # word_vector = ft.get_word_vector(word)\n",
    "                        temp_word.append(word_vector)\n",
    "                        \n",
    "                temp_char = pad_sequences(temp_char, 52)\n",
    "                word_embeddings.append(temp_word)\n",
    "                case_embeddings.append(temp_casing)\n",
    "                char_embeddings.append(temp_char)\n",
    "                temp_output = to_categorical(temp_output, len(label2Idx))\n",
    "                output_labels.append(temp_output)\n",
    "    #             output_labels = to_categorical()\n",
    "    #             output_labels = np.array(output_labels)\n",
    "    #             output_labels = output_labels[...,newaxis]\n",
    "\n",
    "    #             print(np.array(word_embeddings).shape)\n",
    "    #             print(np.array(case_embeddings).shape)\n",
    "    #             print(np.array(char_embeddings).shape)\n",
    "    #             print(output_labels.shape)\n",
    "    #             print(\"******************\\n\\n\")\n",
    "            yield ([np.array(word_embeddings), np.array(case_embeddings), np.array(char_embeddings)], np.array(output_labels))\n",
    "\n",
    "def get_label_from_categorical(a):\n",
    "    labels = []\n",
    "    for label in a:\n",
    "        label = np.ndarray.tolist(label)\n",
    "        label = np.argmax(label)\n",
    "        labels.append(label)\n",
    "    return(labels)\n",
    "\n",
    "def predict_batches(batch):\n",
    "    steps = 0\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for input_data, output_data in generator(batch):\n",
    "        pred_labels_batch = model.predict(input_data)\n",
    "        for s in pred_labels_batch:\n",
    "            pred_labels.append(get_label_from_categorical(s))\n",
    "        for s in output_data:\n",
    "            true_labels.append(get_label_from_categorical(s))\n",
    "        steps += 1\n",
    "        if steps == math.ceil(len(batch) / batch_size):\n",
    "            break\n",
    "    return(true_labels, pred_labels)\n",
    "\n",
    "\n",
    "def predict_batches_ignore_padding(batch):\n",
    "    steps = 0\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for input_data, output_data in generator(batch):\n",
    "        \n",
    "        pred_labels_batch = model.predict(input_data)\n",
    "        for s_id, s in enumerate(output_data):\n",
    "            not_padded_true = []\n",
    "            not_padded_pred = []\n",
    "            predicted_labels = get_label_from_categorical(pred_labels_batch[s_id])\n",
    "            for t_id, t in enumerate(get_label_from_categorical(s)):\n",
    "                if t != 0:\n",
    "                    not_padded_true.append(t)\n",
    "                    not_padded_pred.append(predicted_labels[t_id])\n",
    "            true_labels.append(not_padded_true)\n",
    "            pred_labels.append(not_padded_pred)\n",
    "            \n",
    "        steps += 1\n",
    "        if steps == math.ceil(len(batch) / batch_size):\n",
    "            break\n",
    "    return(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "idx2Label = {v: k for k, v in label2Idx.items()}\n",
    "print(len(label2Idx))\n",
    "print(len(idx2Label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_char_embedding_dims = 52\n",
    "def get_model_lstm():\n",
    "    words_input = Input(shape=(None, nb_embedding_dims), dtype='float32', name='words_input')\n",
    "    casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
    "    casing = Embedding(output_dim=caseEmbeddings.shape[1], input_dim=caseEmbeddings.shape[0], weights=[caseEmbeddings], trainable=False, name = 'case_embed')(casing_input)\n",
    "    character_input=Input(shape=(None,nb_char_embedding_dims,),name='char_input')\n",
    "    embed_char_out=TimeDistributed(Embedding(len(char2Idx),32,embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input)\n",
    "    char_lstm = TimeDistributed(Bidirectional(LSTM(50)))(embed_char_out)\n",
    "    output = concatenate([words_input, casing, char_lstm])\n",
    "    output = Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.5))(output)\n",
    "    output = TimeDistributed(Dense(len(label2Idx)))(output)\n",
    "    crf = CRF(len(label2Idx))\n",
    "    output = crf(output)\n",
    "    model = Model(inputs=[words_input, casing_input, character_input], outputs=[output])\n",
    "    model.compile(loss=crf.loss_function, optimizer='nadam', metrics=[crf.accuracy])\n",
    "    model.summary()\n",
    "    return(model)\n",
    "\n",
    "class F1History(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.acc = []\n",
    "        self.f1_scores = []\n",
    "        self.max_f1 = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.acc.append(logs.get('val_acc'))\n",
    "        true_labels, pred_labels = predict_batches_ignore_padding(devSentences)\n",
    "        pre, rec, f1 = compute_f1(pred_labels, true_labels, idx2Label)\n",
    "        self.f1_scores.append(f1)\n",
    "        if epoch > -1 and f1 > self.max_f1:\n",
    "            print(\"\\nNew maximum F1 score: \" + str(f1) + \" (before: \" + str(self.max_f1) + \") Saving to \" + tmp_model_filename)\n",
    "            self.max_f1 = f1\n",
    "            model.save(tmp_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model_filename = 'tmp_generator_NER_lstm_best.h5'\n",
    "# checkpoint = ModelCheckpoint(tmp_model_filename, verbose=1, save_best_only = True, monitor = 'val_acc')\n",
    "history = F1History()\n",
    "model = get_model()\n",
    "model.fit_generator(\n",
    "    generator(trainSentences, shuffle_data=True), steps_per_epoch = math.ceil(len(trainSentences) / batch_size), \n",
    "    validation_data = generator(devSentences), validation_steps = math.ceil(len(devSentences) / batch_size), \n",
    "    epochs = 12, callbacks = [history]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.acc)\n",
    "print(history.f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(tmp_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels, pred_labels = predict_batches_ignore_padding(testSentences)\n",
    "print(compute_f1(pred_labels, true_labels, idx2Label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, None, 52)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "casing_input (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, None, 52, 32) 10560       char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "words_input (InputLayer)        (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "case_embed (Embedding)          (None, None, 8)      64          casing_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 100)    33200       char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 408)    0           words_input[0][0]                \n",
      "                                                                 case_embed[0][0]                 \n",
      "                                                                 time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, None, 400)    974400      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, None, 26)     10426       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "crf_1 (CRF)                     (None, None, 26)     1430        time_distributed_2[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 1,030,080\n",
      "Trainable params: 1,030,016\n",
      "Non-trainable params: 64\n",
      "__________________________________________________________________________________________________\n",
      "Creating batches ...Creating batches ...\n",
      "Epoch 1/15\n",
      "\n",
      "Shuffling ...\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.0780 - acc: 0.9813Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.7420965058236273 (before: 0) Saving to tmp_generator_NER_lstm_best.0.h5\n",
      "750/750 [==============================] - 329s 438ms/step - loss: 0.0780 - acc: 0.9813 - val_loss: 0.0353 - val_acc: 0.9884\n",
      "Epoch 2/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9882Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.7816954238559639 (before: 0.7420965058236273) Saving to tmp_generator_NER_lstm_best.0.h5\n",
      "750/750 [==============================] - 331s 441ms/step - loss: 0.0244 - acc: 0.9882 - val_loss: 0.0092 - val_acc: 0.9903\n",
      "Epoch 3/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0013 - acc: 0.9896Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.7884079509567156 (before: 0.7816954238559639) Saving to tmp_generator_NER_lstm_best.0.h5\n",
      "750/750 [==============================] - 364s 485ms/step - loss: -0.0013 - acc: 0.9896 - val_loss: -0.0147 - val_acc: 0.9905\n",
      "Epoch 4/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0277 - acc: 0.9906Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8055090266145543 (before: 0.7884079509567156) Saving to tmp_generator_NER_lstm_best.0.h5\n",
      "750/750 [==============================] - 367s 490ms/step - loss: -0.0277 - acc: 0.9906 - val_loss: -0.0405 - val_acc: 0.9911\n",
      "Epoch 5/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0548 - acc: 0.9914Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8077496274217586 (before: 0.8055090266145543) Saving to tmp_generator_NER_lstm_best.0.h5\n",
      "750/750 [==============================] - 356s 474ms/step - loss: -0.0548 - acc: 0.9914 - val_loss: -0.0666 - val_acc: 0.9912\n",
      "Epoch 6/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0826 - acc: 0.9919Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8106201262532492 (before: 0.8077496274217586) Saving to tmp_generator_NER_lstm_best.0.h5\n",
      "750/750 [==============================] - 344s 459ms/step - loss: -0.0826 - acc: 0.9919 - val_loss: -0.0933 - val_acc: 0.9910\n",
      "Epoch 7/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1106 - acc: 0.9926Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8196056753270683 (before: 0.8106201262532492) Saving to tmp_generator_NER_lstm_best.0.h5\n",
      "750/750 [==============================] - 343s 458ms/step - loss: -0.1106 - acc: 0.9926 - val_loss: -0.1201 - val_acc: 0.9916\n",
      "Epoch 8/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1385 - acc: 0.9931Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8210565476190476 (before: 0.8196056753270683) Saving to tmp_generator_NER_lstm_best.0.h5\n",
      "750/750 [==============================] - 343s 458ms/step - loss: -0.1386 - acc: 0.9931 - val_loss: -0.1469 - val_acc: 0.9915\n",
      "Epoch 9/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1663 - acc: 0.9934Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8210797862539156 (before: 0.8210565476190476) Saving to tmp_generator_NER_lstm_best.0.h5\n",
      "750/750 [==============================] - 343s 457ms/step - loss: -0.1663 - acc: 0.9934 - val_loss: -0.1733 - val_acc: 0.9915\n",
      "Epoch 10/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1940 - acc: 0.9938Creating batches ...\n",
      "750/750 [==============================] - 343s 458ms/step - loss: -0.1940 - acc: 0.9938 - val_loss: -0.1995 - val_acc: 0.9914\n",
      "Epoch 11/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2214 - acc: 0.9940Creating batches ...\n",
      "750/750 [==============================] - 344s 459ms/step - loss: -0.2214 - acc: 0.9940 - val_loss: -0.2267 - val_acc: 0.9917\n",
      "Epoch 12/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2490 - acc: 0.9943Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8221853694490511 (before: 0.8210797862539156) Saving to tmp_generator_NER_lstm_best.0.h5\n",
      "750/750 [==============================] - 345s 460ms/step - loss: -0.2490 - acc: 0.9943 - val_loss: -0.2527 - val_acc: 0.9919\n",
      "Epoch 13/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2763 - acc: 0.9946Creating batches ...\n",
      "750/750 [==============================] - 345s 459ms/step - loss: -0.2763 - acc: 0.9946 - val_loss: -0.2788 - val_acc: 0.9917\n",
      "Epoch 14/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.3035 - acc: 0.9946Creating batches ...\n",
      "750/750 [==============================] - 346s 462ms/step - loss: -0.3035 - acc: 0.9946 - val_loss: -0.3049 - val_acc: 0.9913\n",
      "Epoch 15/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.3307 - acc: 0.9948Creating batches ...\n",
      "750/750 [==============================] - 348s 465ms/step - loss: -0.3307 - acc: 0.9948 - val_loss: -0.3322 - val_acc: 0.9913\n",
      "Creating batches ...\n",
      "Run 1\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, None, 52)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "casing_input (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, None, 52, 32) 10560       char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "words_input (InputLayer)        (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "case_embed (Embedding)          (None, None, 8)      64          casing_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, None, 100)    33200       char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, 408)    0           words_input[0][0]                \n",
      "                                                                 case_embed[0][0]                 \n",
      "                                                                 time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, None, 400)    974400      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, None, 26)     10426       bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "crf_2 (CRF)                     (None, None, 26)     1430        time_distributed_4[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 1,030,080\n",
      "Trainable params: 1,030,016\n",
      "Non-trainable params: 64\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating batches ...Creating batches ...\n",
      "\n",
      "Shuffling ...Epoch 1/15\n",
      "\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9812Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.743232484076433 (before: 0) Saving to tmp_generator_NER_lstm_best.1.h5\n",
      "750/750 [==============================] - 357s 477ms/step - loss: 0.0701 - acc: 0.9812 - val_loss: 0.0259 - val_acc: 0.9883\n",
      "Epoch 2/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9882Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.783560591511427 (before: 0.743232484076433) Saving to tmp_generator_NER_lstm_best.1.h5\n",
      "750/750 [==============================] - 342s 457ms/step - loss: 0.0173 - acc: 0.9882 - val_loss: 0.0017 - val_acc: 0.9902\n",
      "Epoch 3/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0079 - acc: 0.9896Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.7965451055662188 (before: 0.783560591511427) Saving to tmp_generator_NER_lstm_best.1.h5\n",
      "750/750 [==============================] - 340s 453ms/step - loss: -0.0079 - acc: 0.9896 - val_loss: -0.0224 - val_acc: 0.9907\n",
      "Epoch 4/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0342 - acc: 0.9907Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.806627753718697 (before: 0.7965451055662188) Saving to tmp_generator_NER_lstm_best.1.h5\n",
      "750/750 [==============================] - 344s 459ms/step - loss: -0.0342 - acc: 0.9907 - val_loss: -0.0476 - val_acc: 0.9907\n",
      "Epoch 5/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0613 - acc: 0.9914Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8103061986557133 (before: 0.806627753718697) Saving to tmp_generator_NER_lstm_best.1.h5\n",
      "750/750 [==============================] - 345s 460ms/step - loss: -0.0613 - acc: 0.9914 - val_loss: -0.0734 - val_acc: 0.9909\n",
      "Epoch 6/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0890 - acc: 0.9921Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8122335495829471 (before: 0.8103061986557133) Saving to tmp_generator_NER_lstm_best.1.h5\n",
      "750/750 [==============================] - 345s 460ms/step - loss: -0.0890 - acc: 0.9921 - val_loss: -0.0999 - val_acc: 0.9906\n",
      "Epoch 7/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1167 - acc: 0.9925Creating batches ...\n",
      "750/750 [==============================] - 346s 461ms/step - loss: -0.1167 - acc: 0.9925 - val_loss: -0.1261 - val_acc: 0.9905\n",
      "Epoch 8/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1446 - acc: 0.9929Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8184001477923517 (before: 0.8122335495829471) Saving to tmp_generator_NER_lstm_best.1.h5\n",
      "750/750 [==============================] - 343s 457ms/step - loss: -0.1446 - acc: 0.9929 - val_loss: -0.1528 - val_acc: 0.9910\n",
      "Epoch 9/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1722 - acc: 0.9933Creating batches ...\n",
      "750/750 [==============================] - 349s 466ms/step - loss: -0.1722 - acc: 0.9933 - val_loss: -0.1793 - val_acc: 0.9909\n",
      "Epoch 10/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1998 - acc: 0.9936Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8228037383177571 (before: 0.8184001477923517) Saving to tmp_generator_NER_lstm_best.1.h5\n",
      "750/750 [==============================] - 346s 461ms/step - loss: -0.1998 - acc: 0.9936 - val_loss: -0.2065 - val_acc: 0.9913\n",
      "Epoch 11/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2275 - acc: 0.9939Creating batches ...\n",
      "750/750 [==============================] - 345s 460ms/step - loss: -0.2276 - acc: 0.9939 - val_loss: -0.2327 - val_acc: 0.9912\n",
      "Epoch 12/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2549 - acc: 0.9942Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8240963855421687 (before: 0.8228037383177571) Saving to tmp_generator_NER_lstm_best.1.h5\n",
      "750/750 [==============================] - 347s 463ms/step - loss: -0.2550 - acc: 0.9942 - val_loss: -0.2591 - val_acc: 0.9910\n",
      "Epoch 13/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2823 - acc: 0.9945Creating batches ...\n",
      "750/750 [==============================] - 342s 456ms/step - loss: -0.2823 - acc: 0.9945 - val_loss: -0.2849 - val_acc: 0.9907\n",
      "Epoch 14/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.3093 - acc: 0.9945Creating batches ...\n",
      "750/750 [==============================] - 351s 468ms/step - loss: -0.3093 - acc: 0.9945 - val_loss: -0.3118 - val_acc: 0.9912\n",
      "Epoch 15/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.3367 - acc: 0.9948Creating batches ...\n",
      "750/750 [==============================] - 350s 467ms/step - loss: -0.3368 - acc: 0.9948 - val_loss: -0.3374 - val_acc: 0.9909\n",
      "Creating batches ...\n",
      "Run 2\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, None, 52)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "casing_input (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, None, 52, 32) 10560       char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "words_input (InputLayer)        (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "case_embed (Embedding)          (None, None, 8)      64          casing_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, None, 100)    33200       char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, 408)    0           words_input[0][0]                \n",
      "                                                                 case_embed[0][0]                 \n",
      "                                                                 time_distributed_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, None, 400)    974400      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, None, 26)     10426       bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "crf_3 (CRF)                     (None, None, 26)     1430        time_distributed_6[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 1,030,080\n",
      "Trainable params: 1,030,016\n",
      "Non-trainable params: 64\n",
      "__________________________________________________________________________________________________\n",
      "Creating batches ...\n",
      "Creating batches ...\n",
      "Epoch 1/15Shuffling ...\n",
      "\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9809Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.7423833397608947 (before: 0) Saving to tmp_generator_NER_lstm_best.2.h5\n",
      "750/750 [==============================] - 364s 485ms/step - loss: 0.0686 - acc: 0.9809 - val_loss: 0.0234 - val_acc: 0.9882\n",
      "Epoch 2/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9882Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.7785565448001516 (before: 0.7423833397608947) Saving to tmp_generator_NER_lstm_best.2.h5\n",
      "750/750 [==============================] - 348s 464ms/step - loss: 0.0141 - acc: 0.9882 - val_loss: -0.0011 - val_acc: 0.9897\n",
      "Epoch 3/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0115 - acc: 0.9896Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.7869402985074627 (before: 0.7785565448001516) Saving to tmp_generator_NER_lstm_best.2.h5\n",
      "750/750 [==============================] - 356s 475ms/step - loss: -0.0115 - acc: 0.9896 - val_loss: -0.0247 - val_acc: 0.9900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0380 - acc: 0.9905Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.796759941089838 (before: 0.7869402985074627) Saving to tmp_generator_NER_lstm_best.2.h5\n",
      "750/750 [==============================] - 348s 463ms/step - loss: -0.0380 - acc: 0.9905 - val_loss: -0.0499 - val_acc: 0.9898\n",
      "Epoch 5/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0653 - acc: 0.9913Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8008825151682294 (before: 0.796759941089838) Saving to tmp_generator_NER_lstm_best.2.h5\n",
      "750/750 [==============================] - 354s 472ms/step - loss: -0.0653 - acc: 0.9913 - val_loss: -0.0758 - val_acc: 0.9898\n",
      "Epoch 6/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0931 - acc: 0.9920Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8073022312373225 (before: 0.8008825151682294) Saving to tmp_generator_NER_lstm_best.2.h5\n",
      "750/750 [==============================] - 348s 464ms/step - loss: -0.0932 - acc: 0.9920 - val_loss: -0.1030 - val_acc: 0.9901\n",
      "Epoch 7/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1210 - acc: 0.9924Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8093755722395165 (before: 0.8073022312373225) Saving to tmp_generator_NER_lstm_best.2.h5\n",
      "750/750 [==============================] - 342s 456ms/step - loss: -0.1210 - acc: 0.9924 - val_loss: -0.1297 - val_acc: 0.9902\n",
      "Epoch 8/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1489 - acc: 0.9928Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8131747483989021 (before: 0.8093755722395165) Saving to tmp_generator_NER_lstm_best.2.h5\n",
      "750/750 [==============================] - 342s 456ms/step - loss: -0.1489 - acc: 0.9928 - val_loss: -0.1561 - val_acc: 0.9903\n",
      "Epoch 9/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1767 - acc: 0.9933Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8149237272560191 (before: 0.8131747483989021) Saving to tmp_generator_NER_lstm_best.2.h5\n",
      "750/750 [==============================] - 346s 462ms/step - loss: -0.1767 - acc: 0.9933 - val_loss: -0.1826 - val_acc: 0.9905\n",
      "Epoch 10/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2041 - acc: 0.9936Creating batches ...\n",
      "750/750 [==============================] - 344s 458ms/step - loss: -0.2041 - acc: 0.9936 - val_loss: -0.2093 - val_acc: 0.9906\n",
      "Epoch 11/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2318 - acc: 0.9939Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8180306669129873 (before: 0.8149237272560191) Saving to tmp_generator_NER_lstm_best.2.h5\n",
      "750/750 [==============================] - 346s 462ms/step - loss: -0.2318 - acc: 0.9939 - val_loss: -0.2372 - val_acc: 0.9908\n",
      "Epoch 12/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2591 - acc: 0.9942Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8253323485967503 (before: 0.8180306669129873) Saving to tmp_generator_NER_lstm_best.2.h5\n",
      "750/750 [==============================] - 348s 465ms/step - loss: -0.2592 - acc: 0.9942 - val_loss: -0.2633 - val_acc: 0.9914\n",
      "Epoch 13/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2865 - acc: 0.9944Creating batches ...\n",
      "750/750 [==============================] - 342s 456ms/step - loss: -0.2866 - acc: 0.9944 - val_loss: -0.2897 - val_acc: 0.9908\n",
      "Epoch 14/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.3140 - acc: 0.9946Creating batches ...\n",
      "750/750 [==============================] - 344s 458ms/step - loss: -0.3140 - acc: 0.9946 - val_loss: -0.3162 - val_acc: 0.9911\n",
      "Epoch 15/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.3413 - acc: 0.9948Creating batches ...\n",
      "750/750 [==============================] - 342s 456ms/step - loss: -0.3413 - acc: 0.9948 - val_loss: -0.3428 - val_acc: 0.9914\n",
      "Creating batches ...\n",
      "Run 3\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, None, 52)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "casing_input (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, None, 52, 32) 10560       char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "words_input (InputLayer)        (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "case_embed (Embedding)          (None, None, 8)      64          casing_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, None, 100)    33200       char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, None, 408)    0           words_input[0][0]                \n",
      "                                                                 case_embed[0][0]                 \n",
      "                                                                 time_distributed_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, None, 400)    974400      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistrib (None, None, 26)     10426       bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "crf_4 (CRF)                     (None, None, 26)     1430        time_distributed_8[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 1,030,080\n",
      "Trainable params: 1,030,016\n",
      "Non-trainable params: 64\n",
      "__________________________________________________________________________________________________\n",
      "Creating batches ...\n",
      "Creating batches ...\n",
      "Shuffling ...\n",
      "Epoch 1/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9811Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.7405963749756383 (before: 0) Saving to tmp_generator_NER_lstm_best.3.h5\n",
      "750/750 [==============================] - 355s 473ms/step - loss: 0.0740 - acc: 0.9811 - val_loss: 0.0304 - val_acc: 0.9885\n",
      "Epoch 2/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9882Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.7803657362848894 (before: 0.7405963749756383) Saving to tmp_generator_NER_lstm_best.3.h5\n",
      "750/750 [==============================] - 343s 457ms/step - loss: 0.0217 - acc: 0.9882 - val_loss: 0.0057 - val_acc: 0.9900\n",
      "Epoch 3/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0049 - acc: 0.9897Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.7937831690674754 (before: 0.7803657362848894) Saving to tmp_generator_NER_lstm_best.3.h5\n",
      "750/750 [==============================] - 348s 464ms/step - loss: -0.0049 - acc: 0.9897 - val_loss: -0.0190 - val_acc: 0.9905\n",
      "Epoch 4/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0314 - acc: 0.9904Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8020400453343407 (before: 0.7937831690674754) Saving to tmp_generator_NER_lstm_best.3.h5\n",
      "750/750 [==============================] - 344s 459ms/step - loss: -0.0314 - acc: 0.9904 - val_loss: -0.0450 - val_acc: 0.9907\n",
      "Epoch 5/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0594 - acc: 0.9914Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8029306781889912 (before: 0.8020400453343407) Saving to tmp_generator_NER_lstm_best.3.h5\n",
      "750/750 [==============================] - 346s 462ms/step - loss: -0.0595 - acc: 0.9914 - val_loss: -0.0711 - val_acc: 0.9908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0874 - acc: 0.9921Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8114861085213501 (before: 0.8029306781889912) Saving to tmp_generator_NER_lstm_best.3.h5\n",
      "750/750 [==============================] - 346s 461ms/step - loss: -0.0875 - acc: 0.9921 - val_loss: -0.0978 - val_acc: 0.9911\n",
      "Epoch 7/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1152 - acc: 0.9925Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8160270880361172 (before: 0.8114861085213501) Saving to tmp_generator_NER_lstm_best.3.h5\n",
      "750/750 [==============================] - 351s 468ms/step - loss: -0.1153 - acc: 0.9925 - val_loss: -0.1247 - val_acc: 0.9911\n",
      "Epoch 8/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1433 - acc: 0.9931Creating batches ...\n",
      "750/750 [==============================] - 344s 459ms/step - loss: -0.1433 - acc: 0.9931 - val_loss: -0.1512 - val_acc: 0.9911\n",
      "Epoch 9/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1709 - acc: 0.9934Creating batches ...\n",
      "750/750 [==============================] - 338s 451ms/step - loss: -0.1709 - acc: 0.9934 - val_loss: -0.1778 - val_acc: 0.9910\n",
      "Epoch 10/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1986 - acc: 0.9937Creating batches ...\n",
      "750/750 [==============================] - 342s 457ms/step - loss: -0.1986 - acc: 0.9937 - val_loss: -0.2048 - val_acc: 0.9910\n",
      "Epoch 11/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2261 - acc: 0.9939Creating batches ...\n",
      "750/750 [==============================] - 355s 473ms/step - loss: -0.2261 - acc: 0.9939 - val_loss: -0.2301 - val_acc: 0.9909\n",
      "Epoch 12/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2536 - acc: 0.9943Creating batches ...\n",
      "750/750 [==============================] - 351s 469ms/step - loss: -0.2537 - acc: 0.9943 - val_loss: -0.2564 - val_acc: 0.9910\n",
      "Epoch 13/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2810 - acc: 0.9945Creating batches ...\n",
      "750/750 [==============================] - 353s 471ms/step - loss: -0.2810 - acc: 0.9945 - val_loss: -0.2836 - val_acc: 0.9913\n",
      "Epoch 14/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.3081 - acc: 0.9946Creating batches ...\n",
      "750/750 [==============================] - 348s 464ms/step - loss: -0.3081 - acc: 0.9946 - val_loss: -0.3089 - val_acc: 0.9910\n",
      "Epoch 15/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.3354 - acc: 0.9948Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8207964601769911 (before: 0.8160270880361172) Saving to tmp_generator_NER_lstm_best.3.h5\n",
      "750/750 [==============================] - 346s 461ms/step - loss: -0.3354 - acc: 0.9948 - val_loss: -0.3372 - val_acc: 0.9912\n",
      "Creating batches ...\n",
      "Run 4\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, None, 52)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "casing_input (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, None, 52, 32) 10560       char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "words_input (InputLayer)        (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "case_embed (Embedding)          (None, None, 8)      64          casing_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistrib (None, None, 100)    33200       char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, None, 408)    0           words_input[0][0]                \n",
      "                                                                 case_embed[0][0]                 \n",
      "                                                                 time_distributed_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, None, 400)    974400      concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_10 (TimeDistri (None, None, 26)     10426       bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "crf_5 (CRF)                     (None, None, 26)     1430        time_distributed_10[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 1,030,080\n",
      "Trainable params: 1,030,016\n",
      "Non-trainable params: 64\n",
      "__________________________________________________________________________________________________\n",
      "Creating batches ...Creating batches ...\n",
      "\n",
      "Epoch 1/15\n",
      "Shuffling ...\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.0752 - acc: 0.9807Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.7371529828706438 (before: 0) Saving to tmp_generator_NER_lstm_best.4.h5\n",
      "750/750 [==============================] - 353s 470ms/step - loss: 0.0752 - acc: 0.9807 - val_loss: 0.0300 - val_acc: 0.9880\n",
      "Epoch 2/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9881Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.7786438035853469 (before: 0.7371529828706438) Saving to tmp_generator_NER_lstm_best.4.h5\n",
      "750/750 [==============================] - 335s 447ms/step - loss: 0.0215 - acc: 0.9881 - val_loss: 0.0073 - val_acc: 0.9900\n",
      "Epoch 3/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0025 - acc: 0.9897Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.7894635646990963 (before: 0.7786438035853469) Saving to tmp_generator_NER_lstm_best.4.h5\n",
      "750/750 [==============================] - 331s 442ms/step - loss: -0.0025 - acc: 0.9897 - val_loss: -0.0156 - val_acc: 0.9906\n",
      "Epoch 4/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0276 - acc: 0.9906Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.7996915365336417 (before: 0.7894635646990963) Saving to tmp_generator_NER_lstm_best.4.h5\n",
      "750/750 [==============================] - 333s 445ms/step - loss: -0.0276 - acc: 0.9906 - val_loss: -0.0397 - val_acc: 0.9908\n",
      "Epoch 5/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0540 - acc: 0.9913Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8113028636449839 (before: 0.7996915365336417) Saving to tmp_generator_NER_lstm_best.4.h5\n",
      "750/750 [==============================] - 332s 443ms/step - loss: -0.0540 - acc: 0.9913 - val_loss: -0.0657 - val_acc: 0.9911\n",
      "Epoch 6/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0813 - acc: 0.9919Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8187919463087249 (before: 0.8113028636449839) Saving to tmp_generator_NER_lstm_best.4.h5\n",
      "750/750 [==============================] - 335s 447ms/step - loss: -0.0814 - acc: 0.9919 - val_loss: -0.0920 - val_acc: 0.9914\n",
      "Epoch 7/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1093 - acc: 0.9925Creating batches ...\n",
      "750/750 [==============================] - 339s 452ms/step - loss: -0.1093 - acc: 0.9924 - val_loss: -0.1184 - val_acc: 0.9913\n",
      "Epoch 8/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1370 - acc: 0.9931Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8253489249339873 (before: 0.8187919463087249) Saving to tmp_generator_NER_lstm_best.4.h5\n",
      "750/750 [==============================] - 330s 441ms/step - loss: -0.1370 - acc: 0.9931 - val_loss: -0.1451 - val_acc: 0.9916\n",
      "Epoch 9/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1645 - acc: 0.9933Creating batches ...\n",
      "750/750 [==============================] - 339s 452ms/step - loss: -0.1645 - acc: 0.9933 - val_loss: -0.1723 - val_acc: 0.9913\n",
      "Epoch 10/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1924 - acc: 0.9938Creating batches ...\n",
      "750/750 [==============================] - 339s 452ms/step - loss: -0.1924 - acc: 0.9938 - val_loss: -0.1984 - val_acc: 0.9913\n",
      "Epoch 11/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2197 - acc: 0.9940Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8269696405289626 (before: 0.8253489249339873) Saving to tmp_generator_NER_lstm_best.4.h5\n",
      "750/750 [==============================] - 340s 454ms/step - loss: -0.2197 - acc: 0.9940 - val_loss: -0.2250 - val_acc: 0.9917\n",
      "Epoch 12/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2470 - acc: 0.9942Creating batches ...\n",
      "750/750 [==============================] - 334s 445ms/step - loss: -0.2471 - acc: 0.9942 - val_loss: -0.2509 - val_acc: 0.9914\n",
      "Epoch 13/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2743 - acc: 0.9943Creating batches ...\n",
      "750/750 [==============================] - 321s 428ms/step - loss: -0.2743 - acc: 0.9943 - val_loss: -0.2776 - val_acc: 0.9912\n",
      "Epoch 14/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.3017 - acc: 0.9946Creating batches ...\n",
      "750/750 [==============================] - 320s 427ms/step - loss: -0.3017 - acc: 0.9946 - val_loss: -0.3038 - val_acc: 0.9911\n",
      "Epoch 15/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.3291 - acc: 0.9949Creating batches ...\n",
      "750/750 [==============================] - 319s 425ms/step - loss: -0.3292 - acc: 0.9949 - val_loss: -0.3297 - val_acc: 0.9913\n",
      "Creating batches ...\n",
      "Run 5\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, None, 52)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "casing_input (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, None, 52, 32) 10560       char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "words_input (InputLayer)        (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "case_embed (Embedding)          (None, None, 8)      64          casing_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_11 (TimeDistri (None, None, 100)    33200       char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, None, 408)    0           words_input[0][0]                \n",
      "                                                                 case_embed[0][0]                 \n",
      "                                                                 time_distributed_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional (None, None, 400)    974400      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_12 (TimeDistri (None, None, 26)     10426       bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "crf_6 (CRF)                     (None, None, 26)     1430        time_distributed_12[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 1,030,080\n",
      "Trainable params: 1,030,016\n",
      "Non-trainable params: 64\n",
      "__________________________________________________________________________________________________\n",
      "Creating batches ...\n",
      "Creating batches ...\n",
      "Epoch 1/15\n",
      "Shuffling ...\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9806Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.734788487541047 (before: 0) Saving to tmp_generator_NER_lstm_best.5.h5\n",
      "750/750 [==============================] - 337s 450ms/step - loss: 0.0774 - acc: 0.9806 - val_loss: 0.0305 - val_acc: 0.9881\n",
      "Epoch 2/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9881Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.7765609990393851 (before: 0.734788487541047) Saving to tmp_generator_NER_lstm_best.5.h5\n",
      "750/750 [==============================] - 328s 437ms/step - loss: 0.0222 - acc: 0.9881 - val_loss: 0.0070 - val_acc: 0.9896\n",
      "Epoch 3/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0032 - acc: 0.9894Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.7886735081717976 (before: 0.7765609990393851) Saving to tmp_generator_NER_lstm_best.5.h5\n",
      "750/750 [==============================] - 345s 460ms/step - loss: -0.0032 - acc: 0.9894 - val_loss: -0.0171 - val_acc: 0.9902\n",
      "Epoch 4/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0291 - acc: 0.9903Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.7999242137173171 (before: 0.7886735081717976) Saving to tmp_generator_NER_lstm_best.5.h5\n",
      "750/750 [==============================] - 341s 455ms/step - loss: -0.0292 - acc: 0.9904 - val_loss: -0.0424 - val_acc: 0.9904\n",
      "Epoch 5/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0563 - acc: 0.9912Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.806766917293233 (before: 0.7999242137173171) Saving to tmp_generator_NER_lstm_best.5.h5\n",
      "750/750 [==============================] - 340s 453ms/step - loss: -0.0564 - acc: 0.9912 - val_loss: -0.0685 - val_acc: 0.9909\n",
      "Epoch 6/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0840 - acc: 0.9918Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8161806208842897 (before: 0.806766917293233) Saving to tmp_generator_NER_lstm_best.5.h5\n",
      "750/750 [==============================] - 343s 457ms/step - loss: -0.0840 - acc: 0.9918 - val_loss: -0.0955 - val_acc: 0.9911\n",
      "Epoch 7/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1119 - acc: 0.9924Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8238185255198488 (before: 0.8161806208842897) Saving to tmp_generator_NER_lstm_best.5.h5\n",
      "750/750 [==============================] - 341s 455ms/step - loss: -0.1119 - acc: 0.9924 - val_loss: -0.1219 - val_acc: 0.9915\n",
      "Epoch 8/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1398 - acc: 0.9928Creating batches ...\n",
      "750/750 [==============================] - 343s 457ms/step - loss: -0.1398 - acc: 0.9928 - val_loss: -0.1485 - val_acc: 0.9911\n",
      "Epoch 9/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1673 - acc: 0.9930Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8276511584102467 (before: 0.8238185255198488) Saving to tmp_generator_NER_lstm_best.5.h5\n",
      "750/750 [==============================] - 336s 448ms/step - loss: -0.1673 - acc: 0.9930 - val_loss: -0.1757 - val_acc: 0.9918\n",
      "Epoch 10/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1951 - acc: 0.9935Creating batches ...\n",
      "750/750 [==============================] - 330s 440ms/step - loss: -0.1951 - acc: 0.9935 - val_loss: -0.2020 - val_acc: 0.9915\n",
      "Epoch 11/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2228 - acc: 0.9938Creating batches ...\n",
      "750/750 [==============================] - 323s 431ms/step - loss: -0.2228 - acc: 0.9938 - val_loss: -0.2282 - val_acc: 0.9913\n",
      "Epoch 12/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2503 - acc: 0.9942Creating batches ...\n",
      "750/750 [==============================] - 321s 428ms/step - loss: -0.2503 - acc: 0.9942 - val_loss: -0.2549 - val_acc: 0.9911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2776 - acc: 0.9943Creating batches ...\n",
      "750/750 [==============================] - 322s 429ms/step - loss: -0.2776 - acc: 0.9943 - val_loss: -0.2811 - val_acc: 0.9913\n",
      "Epoch 14/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.3050 - acc: 0.9946Creating batches ...\n",
      "750/750 [==============================] - 321s 428ms/step - loss: -0.3050 - acc: 0.9946 - val_loss: -0.3076 - val_acc: 0.9914\n",
      "Epoch 15/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.3321 - acc: 0.9948Creating batches ...\n",
      "750/750 [==============================] - 322s 429ms/step - loss: -0.3322 - acc: 0.9948 - val_loss: -0.3345 - val_acc: 0.9911\n",
      "Creating batches ...\n",
      "Run 6\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, None, 52)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "casing_input (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, None, 52, 32) 10560       char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "words_input (InputLayer)        (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "case_embed (Embedding)          (None, None, 8)      64          casing_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_13 (TimeDistri (None, None, 100)    33200       char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, None, 408)    0           words_input[0][0]                \n",
      "                                                                 case_embed[0][0]                 \n",
      "                                                                 time_distributed_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_14 (Bidirectional (None, None, 400)    974400      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_14 (TimeDistri (None, None, 26)     10426       bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "crf_7 (CRF)                     (None, None, 26)     1430        time_distributed_14[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 1,030,080\n",
      "Trainable params: 1,030,016\n",
      "Non-trainable params: 64\n",
      "__________________________________________________________________________________________________\n",
      "Creating batches ...Creating batches ...Epoch 1/15\n",
      "\n",
      "\n",
      "Shuffling ...\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9809Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.7400386847195359 (before: 0) Saving to tmp_generator_NER_lstm_best.6.h5\n",
      "750/750 [==============================] - 339s 452ms/step - loss: 0.0788 - acc: 0.9809 - val_loss: 0.0347 - val_acc: 0.9883\n",
      "Epoch 2/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9881Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.777545195052331 (before: 0.7400386847195359) Saving to tmp_generator_NER_lstm_best.6.h5\n",
      "750/750 [==============================] - 322s 429ms/step - loss: 0.0258 - acc: 0.9881 - val_loss: 0.0108 - val_acc: 0.9899\n",
      "Epoch 3/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: 6.9902e-04 - acc: 0.9895Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.7956708341108417 (before: 0.777545195052331) Saving to tmp_generator_NER_lstm_best.6.h5\n",
      "750/750 [==============================] - 322s 429ms/step - loss: 6.9966e-04 - acc: 0.9895 - val_loss: -0.0127 - val_acc: 0.9905\n",
      "Epoch 4/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0253 - acc: 0.9905Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.7988077496274217 (before: 0.7956708341108417) Saving to tmp_generator_NER_lstm_best.6.h5\n",
      "750/750 [==============================] - 320s 427ms/step - loss: -0.0253 - acc: 0.9905 - val_loss: -0.0375 - val_acc: 0.9906\n",
      "Epoch 5/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0522 - acc: 0.9913Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8014027316352897 (before: 0.7988077496274217) Saving to tmp_generator_NER_lstm_best.6.h5\n",
      "750/750 [==============================] - 323s 430ms/step - loss: -0.0522 - acc: 0.9913 - val_loss: -0.0634 - val_acc: 0.9904\n",
      "Epoch 6/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0798 - acc: 0.9919Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8115568641884432 (before: 0.8014027316352897) Saving to tmp_generator_NER_lstm_best.6.h5\n",
      "750/750 [==============================] - 321s 428ms/step - loss: -0.0798 - acc: 0.9919 - val_loss: -0.0900 - val_acc: 0.9907\n",
      "Epoch 7/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1077 - acc: 0.9924Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8184342032204333 (before: 0.8115568641884432) Saving to tmp_generator_NER_lstm_best.6.h5\n",
      "750/750 [==============================] - 322s 429ms/step - loss: -0.1077 - acc: 0.9924 - val_loss: -0.1167 - val_acc: 0.9912\n",
      "Epoch 8/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1355 - acc: 0.9930Creating batches ...\n",
      "750/750 [==============================] - 321s 429ms/step - loss: -0.1355 - acc: 0.9930 - val_loss: -0.1432 - val_acc: 0.9910\n",
      "Epoch 9/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1632 - acc: 0.9933Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8189987163029525 (before: 0.8184342032204333) Saving to tmp_generator_NER_lstm_best.6.h5\n",
      "750/750 [==============================] - 322s 429ms/step - loss: -0.1632 - acc: 0.9933 - val_loss: -0.1693 - val_acc: 0.9912\n",
      "Epoch 10/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1908 - acc: 0.9937Creating batches ...\n",
      "750/750 [==============================] - 322s 430ms/step - loss: -0.1908 - acc: 0.9937 - val_loss: -0.1956 - val_acc: 0.9911\n",
      "Epoch 11/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2185 - acc: 0.9940Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8235510579576817 (before: 0.8189987163029525) Saving to tmp_generator_NER_lstm_best.6.h5\n",
      "750/750 [==============================] - 324s 433ms/step - loss: -0.2185 - acc: 0.9940 - val_loss: -0.2230 - val_acc: 0.9914\n",
      "Epoch 12/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2459 - acc: 0.9942Creating batches ...\n",
      "750/750 [==============================] - 325s 434ms/step - loss: -0.2459 - acc: 0.9942 - val_loss: -0.2485 - val_acc: 0.9913\n",
      "Epoch 13/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2730 - acc: 0.9943Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8245002750779388 (before: 0.8235510579576817) Saving to tmp_generator_NER_lstm_best.6.h5\n",
      "750/750 [==============================] - 327s 436ms/step - loss: -0.2730 - acc: 0.9943 - val_loss: -0.2755 - val_acc: 0.9915\n",
      "Epoch 14/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.3006 - acc: 0.9946Creating batches ...\n",
      "750/750 [==============================] - 322s 429ms/step - loss: -0.3006 - acc: 0.9946 - val_loss: -0.3017 - val_acc: 0.9912\n",
      "Epoch 15/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.3276 - acc: 0.9947Creating batches ...\n",
      "750/750 [==============================] - 324s 432ms/step - loss: -0.3276 - acc: 0.9947 - val_loss: -0.3281 - val_acc: 0.9911\n",
      "Creating batches ...\n",
      "Run 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, None, 52)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "casing_input (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, None, 52, 32) 10560       char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "words_input (InputLayer)        (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "case_embed (Embedding)          (None, None, 8)      64          casing_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_15 (TimeDistri (None, None, 100)    33200       char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, None, 408)    0           words_input[0][0]                \n",
      "                                                                 case_embed[0][0]                 \n",
      "                                                                 time_distributed_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_16 (Bidirectional (None, None, 400)    974400      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_16 (TimeDistri (None, None, 26)     10426       bidirectional_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "crf_8 (CRF)                     (None, None, 26)     1430        time_distributed_16[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 1,030,080\n",
      "Trainable params: 1,030,016\n",
      "Non-trainable params: 64\n",
      "__________________________________________________________________________________________________\n",
      "Creating batches ...\n",
      "Creating batches ...\n",
      "Epoch 1/15Shuffling ...\n",
      "\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9815Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.7449703008239126 (before: 0) Saving to tmp_generator_NER_lstm_best.7.h5\n",
      "750/750 [==============================] - 336s 449ms/step - loss: 0.0750 - acc: 0.9815 - val_loss: 0.0289 - val_acc: 0.9885\n",
      "Epoch 2/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9881Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.7811969039078723 (before: 0.7449703008239126) Saving to tmp_generator_NER_lstm_best.7.h5\n",
      "750/750 [==============================] - 321s 428ms/step - loss: 0.0209 - acc: 0.9881 - val_loss: 0.0050 - val_acc: 0.9901\n",
      "Epoch 3/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0047 - acc: 0.9895Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.7926371149511646 (before: 0.7811969039078723) Saving to tmp_generator_NER_lstm_best.7.h5\n",
      "750/750 [==============================] - 320s 427ms/step - loss: -0.0047 - acc: 0.9895 - val_loss: -0.0188 - val_acc: 0.9905\n",
      "Epoch 4/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0309 - acc: 0.9904Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8014118521270668 (before: 0.7926371149511646) Saving to tmp_generator_NER_lstm_best.7.h5\n",
      "750/750 [==============================] - 317s 423ms/step - loss: -0.0309 - acc: 0.9904 - val_loss: -0.0440 - val_acc: 0.9907\n",
      "Epoch 5/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0581 - acc: 0.9912Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8153530836594 (before: 0.8014118521270668) Saving to tmp_generator_NER_lstm_best.7.h5\n",
      "750/750 [==============================] - 320s 426ms/step - loss: -0.0581 - acc: 0.9912 - val_loss: -0.0704 - val_acc: 0.9911\n",
      "Epoch 6/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0859 - acc: 0.9918Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8226656626506024 (before: 0.8153530836594) Saving to tmp_generator_NER_lstm_best.7.h5\n",
      "750/750 [==============================] - 318s 424ms/step - loss: -0.0859 - acc: 0.9918 - val_loss: -0.0974 - val_acc: 0.9913\n",
      "Epoch 7/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1138 - acc: 0.9923Creating batches ...\n",
      "750/750 [==============================] - 319s 425ms/step - loss: -0.1138 - acc: 0.9923 - val_loss: -0.1238 - val_acc: 0.9912\n",
      "Epoch 8/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1415 - acc: 0.9929Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8268177525967894 (before: 0.8226656626506024) Saving to tmp_generator_NER_lstm_best.7.h5\n",
      "750/750 [==============================] - 319s 425ms/step - loss: -0.1415 - acc: 0.9929 - val_loss: -0.1506 - val_acc: 0.9916\n",
      "Epoch 9/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1694 - acc: 0.9932Creating batches ...\n",
      "750/750 [==============================] - 318s 424ms/step - loss: -0.1694 - acc: 0.9932 - val_loss: -0.1769 - val_acc: 0.9914\n",
      "Epoch 10/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1972 - acc: 0.9936Creating batches ...\n",
      "750/750 [==============================] - 320s 426ms/step - loss: -0.1972 - acc: 0.9936 - val_loss: -0.2036 - val_acc: 0.9914\n",
      "Epoch 11/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2248 - acc: 0.9940Creating batches ...\n",
      "750/750 [==============================] - 319s 425ms/step - loss: -0.2248 - acc: 0.9940 - val_loss: -0.2302 - val_acc: 0.9912\n",
      "Epoch 12/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2521 - acc: 0.9942Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8270168855534709 (before: 0.8268177525967894) Saving to tmp_generator_NER_lstm_best.7.h5\n",
      "750/750 [==============================] - 319s 425ms/step - loss: -0.2521 - acc: 0.9942 - val_loss: -0.2571 - val_acc: 0.9914\n",
      "Epoch 13/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2795 - acc: 0.9945Creating batches ...\n",
      "750/750 [==============================] - 318s 423ms/step - loss: -0.2795 - acc: 0.9945 - val_loss: -0.2834 - val_acc: 0.9916\n",
      "Epoch 14/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.3068 - acc: 0.9947Creating batches ...\n",
      "750/750 [==============================] - 322s 429ms/step - loss: -0.3068 - acc: 0.9947 - val_loss: -0.3097 - val_acc: 0.9913\n",
      "Epoch 15/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.3340 - acc: 0.9947Creating batches ...\n",
      "750/750 [==============================] - 324s 432ms/step - loss: -0.3340 - acc: 0.9947 - val_loss: -0.3357 - val_acc: 0.9912\n",
      "Creating batches ...\n",
      "Run 8\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, None, 52)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "casing_input (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, None, 52, 32) 10560       char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "words_input (InputLayer)        (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "case_embed (Embedding)          (None, None, 8)      64          casing_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_17 (TimeDistri (None, None, 100)    33200       char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, None, 408)    0           words_input[0][0]                \n",
      "                                                                 case_embed[0][0]                 \n",
      "                                                                 time_distributed_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_18 (Bidirectional (None, None, 400)    974400      concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_18 (TimeDistri (None, None, 26)     10426       bidirectional_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "crf_9 (CRF)                     (None, None, 26)     1430        time_distributed_18[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 1,030,080\n",
      "Trainable params: 1,030,016\n",
      "Non-trainable params: 64\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating batches ...Creating batches ...\n",
      "Epoch 1/15\n",
      "\n",
      "Shuffling ...\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9811Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.7523659305993691 (before: 0) Saving to tmp_generator_NER_lstm_best.8.h5\n",
      "750/750 [==============================] - 337s 450ms/step - loss: 0.0740 - acc: 0.9811 - val_loss: 0.0275 - val_acc: 0.9885\n",
      "Epoch 2/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9879Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.7791056283731688 (before: 0.7523659305993691) Saving to tmp_generator_NER_lstm_best.8.h5\n",
      "750/750 [==============================] - 319s 425ms/step - loss: 0.0194 - acc: 0.9879 - val_loss: 0.0029 - val_acc: 0.9900\n",
      "Epoch 3/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0066 - acc: 0.9895Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.7990832696715049 (before: 0.7791056283731688) Saving to tmp_generator_NER_lstm_best.8.h5\n",
      "750/750 [==============================] - 319s 425ms/step - loss: -0.0066 - acc: 0.9895 - val_loss: -0.0213 - val_acc: 0.9906\n",
      "Epoch 4/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0332 - acc: 0.9906Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8043765327296737 (before: 0.7990832696715049) Saving to tmp_generator_NER_lstm_best.8.h5\n",
      "750/750 [==============================] - 319s 425ms/step - loss: -0.0332 - acc: 0.9906 - val_loss: -0.0464 - val_acc: 0.9906\n",
      "Epoch 5/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0604 - acc: 0.9911Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8084946438639353 (before: 0.8043765327296737) Saving to tmp_generator_NER_lstm_best.8.h5\n",
      "750/750 [==============================] - 320s 426ms/step - loss: -0.0604 - acc: 0.9911 - val_loss: -0.0731 - val_acc: 0.9910\n",
      "Epoch 6/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0882 - acc: 0.9918Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8159819921215532 (before: 0.8084946438639353) Saving to tmp_generator_NER_lstm_best.8.h5\n",
      "750/750 [==============================] - 319s 425ms/step - loss: -0.0882 - acc: 0.9918 - val_loss: -0.0994 - val_acc: 0.9911\n",
      "Epoch 7/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1161 - acc: 0.9923Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8247153257420198 (before: 0.8159819921215532) Saving to tmp_generator_NER_lstm_best.8.h5\n",
      "750/750 [==============================] - 319s 426ms/step - loss: -0.1161 - acc: 0.9923 - val_loss: -0.1260 - val_acc: 0.9914\n",
      "Epoch 8/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1441 - acc: 0.9930Creating batches ...\n",
      "750/750 [==============================] - 319s 426ms/step - loss: -0.1441 - acc: 0.9930 - val_loss: -0.1528 - val_acc: 0.9913\n",
      "Epoch 9/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1718 - acc: 0.9933Creating batches ...\n",
      "750/750 [==============================] - 320s 427ms/step - loss: -0.1719 - acc: 0.9933 - val_loss: -0.1789 - val_acc: 0.9915\n",
      "Epoch 10/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1994 - acc: 0.9935Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8272897196261683 (before: 0.8247153257420198) Saving to tmp_generator_NER_lstm_best.8.h5\n",
      "750/750 [==============================] - 320s 427ms/step - loss: -0.1994 - acc: 0.9935 - val_loss: -0.2060 - val_acc: 0.9915\n",
      "Epoch 11/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2270 - acc: 0.9939Creating batches ...\n",
      "750/750 [==============================] - 318s 425ms/step - loss: -0.2270 - acc: 0.9939 - val_loss: -0.2316 - val_acc: 0.9913\n",
      "Epoch 12/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2544 - acc: 0.9941Creating batches ...\n",
      "750/750 [==============================] - 322s 429ms/step - loss: -0.2545 - acc: 0.9941 - val_loss: -0.2586 - val_acc: 0.9915\n",
      "Epoch 13/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2819 - acc: 0.9944Creating batches ...\n",
      "750/750 [==============================] - 320s 426ms/step - loss: -0.2819 - acc: 0.9944 - val_loss: -0.2846 - val_acc: 0.9912\n",
      "Epoch 14/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.3091 - acc: 0.9946Creating batches ...\n",
      "750/750 [==============================] - 322s 429ms/step - loss: -0.3091 - acc: 0.9946 - val_loss: -0.3109 - val_acc: 0.9913\n",
      "Epoch 15/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.3365 - acc: 0.9949Creating batches ...\n",
      "750/750 [==============================] - 320s 427ms/step - loss: -0.3365 - acc: 0.9949 - val_loss: -0.3382 - val_acc: 0.9915\n",
      "Creating batches ...\n",
      "Run 9\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, None, 52)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "casing_input (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, None, 52, 32) 10560       char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "words_input (InputLayer)        (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "case_embed (Embedding)          (None, None, 8)      64          casing_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_19 (TimeDistri (None, None, 100)    33200       char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, None, 408)    0           words_input[0][0]                \n",
      "                                                                 case_embed[0][0]                 \n",
      "                                                                 time_distributed_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_20 (Bidirectional (None, None, 400)    974400      concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_20 (TimeDistri (None, None, 26)     10426       bidirectional_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "crf_10 (CRF)                    (None, None, 26)     1430        time_distributed_20[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 1,030,080\n",
      "Trainable params: 1,030,016\n",
      "Non-trainable params: 64\n",
      "__________________________________________________________________________________________________\n",
      "Creating batches ...Creating batches ...\n",
      "\n",
      "Epoch 1/15Shuffling ...\n",
      "\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9813Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.7456295423296014 (before: 0) Saving to tmp_generator_NER_lstm_best.9.h5\n",
      "750/750 [==============================] - 340s 453ms/step - loss: 0.0666 - acc: 0.9813 - val_loss: 0.0222 - val_acc: 0.9885\n",
      "Epoch 2/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9882Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.7827599536142249 (before: 0.7456295423296014) Saving to tmp_generator_NER_lstm_best.9.h5\n",
      "750/750 [==============================] - 323s 430ms/step - loss: 0.0137 - acc: 0.9882 - val_loss: -0.0011 - val_acc: 0.9899\n",
      "Epoch 3/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0109 - acc: 0.9895Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.7949211235090419 (before: 0.7827599536142249) Saving to tmp_generator_NER_lstm_best.9.h5\n",
      "750/750 [==============================] - 325s 434ms/step - loss: -0.0109 - acc: 0.9895 - val_loss: -0.0245 - val_acc: 0.9903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0364 - acc: 0.9905Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8048359240069085 (before: 0.7949211235090419) Saving to tmp_generator_NER_lstm_best.9.h5\n",
      "750/750 [==============================] - 322s 430ms/step - loss: -0.0364 - acc: 0.9905 - val_loss: -0.0497 - val_acc: 0.9908\n",
      "Epoch 5/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0634 - acc: 0.9912Creating batches ...\n",
      "750/750 [==============================] - 321s 428ms/step - loss: -0.0635 - acc: 0.9912 - val_loss: -0.0754 - val_acc: 0.9906\n",
      "Epoch 6/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.0910 - acc: 0.9920Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8150517403574787 (before: 0.8048359240069085) Saving to tmp_generator_NER_lstm_best.9.h5\n",
      "750/750 [==============================] - 320s 427ms/step - loss: -0.0911 - acc: 0.9920 - val_loss: -0.1020 - val_acc: 0.9912\n",
      "Epoch 7/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1187 - acc: 0.9925Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8160724391624222 (before: 0.8150517403574787) Saving to tmp_generator_NER_lstm_best.9.h5\n",
      "750/750 [==============================] - 319s 426ms/step - loss: -0.1187 - acc: 0.9925 - val_loss: -0.1289 - val_acc: 0.9910\n",
      "Epoch 8/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1465 - acc: 0.9930Creating batches ...\n",
      "750/750 [==============================] - 319s 426ms/step - loss: -0.1465 - acc: 0.9930 - val_loss: -0.1555 - val_acc: 0.9908\n",
      "Epoch 9/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.1742 - acc: 0.9934Creating batches ...\n",
      "750/750 [==============================] - 320s 427ms/step - loss: -0.1742 - acc: 0.9934 - val_loss: -0.1823 - val_acc: 0.9912\n",
      "Epoch 10/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2019 - acc: 0.9937Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8182503770739064 (before: 0.8160724391624222) Saving to tmp_generator_NER_lstm_best.9.h5\n",
      "750/750 [==============================] - 321s 428ms/step - loss: -0.2019 - acc: 0.9937 - val_loss: -0.2079 - val_acc: 0.9913\n",
      "Epoch 11/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2293 - acc: 0.9939Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8228014250890681 (before: 0.8182503770739064) Saving to tmp_generator_NER_lstm_best.9.h5\n",
      "750/750 [==============================] - 321s 428ms/step - loss: -0.2294 - acc: 0.9939 - val_loss: -0.2353 - val_acc: 0.9914\n",
      "Epoch 12/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2568 - acc: 0.9941Creating batches ...\n",
      "750/750 [==============================] - 317s 423ms/step - loss: -0.2568 - acc: 0.9941 - val_loss: -0.2616 - val_acc: 0.9914\n",
      "Epoch 13/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.2842 - acc: 0.9944Creating batches ...\n",
      "750/750 [==============================] - 321s 428ms/step - loss: -0.2842 - acc: 0.9944 - val_loss: -0.2881 - val_acc: 0.9915\n",
      "Epoch 14/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.3115 - acc: 0.9945Creating batches ...\n",
      "750/750 [==============================] - 319s 425ms/step - loss: -0.3115 - acc: 0.9945 - val_loss: -0.3145 - val_acc: 0.9914\n",
      "Epoch 15/15\n",
      "749/750 [============================>.] - ETA: 0s - loss: -0.3389 - acc: 0.9947Creating batches ...\n",
      "\n",
      "New maximum F1 score: 0.8238157648380453 (before: 0.8228014250890681) Saving to tmp_generator_NER_lstm_best.9.h5\n",
      "750/750 [==============================] - 319s 425ms/step - loss: -0.3389 - acc: 0.9947 - val_loss: -0.3407 - val_acc: 0.9916\n",
      "Creating batches ...\n"
     ]
    }
   ],
   "source": [
    "f = open('results_lstm.txt', 'w')\n",
    "for run_i in range(10):\n",
    "    print(\"Run \" + str(run_i))\n",
    "    \n",
    "    tmp_model_filename = 'tmp_generator_NER_lstm_best.' + str(run_i) + '.h5'\n",
    "    # tmp_model_filename = 'tmp_generator_NER_best.h5'\n",
    "    # checkpoint = ModelCheckpoint(tmp_model_filename, verbose=1, save_best_only = True, monitor = 'val_acc')\n",
    "    history = F1History()\n",
    "    \n",
    "    model = get_model_lstm()\n",
    "    model.fit_generator(\n",
    "        generator(trainSentences, shuffle_data=True), steps_per_epoch = math.ceil(len(trainSentences) / batch_size), \n",
    "        validation_data = generator(devSentences), validation_steps = math.ceil(len(devSentences) / batch_size), \n",
    "        epochs = 15, callbacks = [history]\n",
    "    )\n",
    "    \n",
    "    model.load_weights(tmp_model_filename)\n",
    "    true_labels, pred_labels = predict_batches_ignore_padding(testSentences)\n",
    "    \n",
    "    pre, rec, f1 = compute_f1(pred_labels, true_labels, idx2Label)\n",
    "    f.write(str(run_i) + \"\\t\" + str(pre) + \"\\t\" + str(rec) +  \"\\t\" + str(f1))\n",
    "    f.write(\"\\n\")\n",
    "    f.flush()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, json\n",
    "# copy file for best run\n",
    "shutil.copyfile('tmp_generator_NER_best.0.h5', 'final_model_germeval.h5')\n",
    "with open(\"final_model_germeval.indexes\", \"w\") as f:\n",
    "    json.dump([idx2Label, label2Idx, char2Idx, case2Idx], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels, pred_labels = predict_batches(test_batches)\n",
    "print(compute_f1(pred_labels, true_labels, idx2Label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testSentences[len(testSentences)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(true_labels[len(testSentences)-1])\n",
    "print(pred_labels[len(testSentences)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2Label[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(testSentences))\n",
    "print(len(pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_batches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
